{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:</br>\n",
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?</br>\n",
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?</br>\n",
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?</br>\n",
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?</br>\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?</br>\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2147483647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../names.txt\", \"r\") as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, 2, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), min(len(w) for w in words), max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E01: train a trigram language model\n",
    "i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Raw 1st approach - 27 x 27 x 27 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "t_list = []\n",
    "for w in words:\n",
    "    chars = [\".\"]*2 + list(w)\n",
    "    for ch1, ch2, ch3 in zip(chars, chars[1:], chars[2:]):\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        t_list.append(trigram)\n",
    "        # print(ch1, ch2)\n",
    "\n",
    "t_counter = Counter(t_list)\n",
    "t = dict(t_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('.', '.', 'a'): 4410,\n",
       "         ('.', '.', 'k'): 2963,\n",
       "         ('.', '.', 'm'): 2538,\n",
       "         ('.', '.', 'j'): 2422,\n",
       "         ('.', '.', 's'): 2055,\n",
       "         ('.', '.', 'd'): 1690,\n",
       "         ('.', '.', 'r'): 1639,\n",
       "         ('.', '.', 'l'): 1572,\n",
       "         ('.', '.', 'c'): 1542,\n",
       "         ('.', '.', 'e'): 1531,\n",
       "         ('.', 'm', 'a'): 1453,\n",
       "         ('.', '.', 't'): 1308,\n",
       "         ('.', '.', 'b'): 1306,\n",
       "         ('.', 'j', 'a'): 1255,\n",
       "         ('.', 'k', 'a'): 1254,\n",
       "         ('.', '.', 'n'): 1146,\n",
       "         ('l', 'y', 'n'): 976,\n",
       "         ('a', 'r', 'i'): 950,\n",
       "         ('.', '.', 'z'): 929,\n",
       "         ('.', '.', 'h'): 874,\n",
       "         ('a', 'n', 'n'): 825,\n",
       "         ('e', 'l', 'l'): 822,\n",
       "         ('a', 'n', 'a'): 804,\n",
       "         ('i', 'a', 'n'): 790,\n",
       "         ('m', 'a', 'r'): 776,\n",
       "         ('a', 'n', 'i'): 703,\n",
       "         ('.', 'd', 'a'): 700,\n",
       "         ('.', '.', 'g'): 669,\n",
       "         ('i', 'y', 'a'): 669,\n",
       "         ('l', 'a', 'n'): 647,\n",
       "         ('.', 'b', 'r'): 646,\n",
       "         ('n', 'n', 'a'): 633,\n",
       "         ('.', 'a', 'l'): 632,\n",
       "         ('.', 'c', 'a'): 628,\n",
       "         ('.', 'a', 'n'): 623,\n",
       "         ('.', 'k', 'e'): 601,\n",
       "         ('a', 'l', 'e'): 601,\n",
       "         ('.', 's', 'a'): 595,\n",
       "         ('.', '.', 'i'): 591,\n",
       "         ('a', 'l', 'i'): 575,\n",
       "         ('s', 'h', 'a'): 562,\n",
       "         ('e', 'l', 'i'): 537,\n",
       "         ('.', '.', 'y'): 535,\n",
       "         ('.', 'd', 'e'): 524,\n",
       "         ('l', 'i', 'a'): 518,\n",
       "         ('l', 'e', 'e'): 517,\n",
       "         ('y', 'n', 'n'): 516,\n",
       "         ('.', '.', 'p'): 515,\n",
       "         ('y', 'a', 'h'): 511,\n",
       "         ('.', 'h', 'a'): 505,\n",
       "         ('l', 'i', 'n'): 505,\n",
       "         ('r', 'i', 'a'): 499,\n",
       "         ('.', 'e', 'l'): 488,\n",
       "         ('a', 'y', 'l'): 483,\n",
       "         ('.', 'a', 'r'): 482,\n",
       "         ('y', 'a', 'n'): 479,\n",
       "         ('.', 'r', 'a'): 472,\n",
       "         ('.', 'n', 'a'): 470,\n",
       "         ('h', 'a', 'n'): 469,\n",
       "         ('.', 'l', 'a'): 463,\n",
       "         ('i', 'a', 'h'): 461,\n",
       "         ('.', 'z', 'a'): 456,\n",
       "         ('l', 'e', 'y'): 443,\n",
       "         ('.', 's', 'h'): 434,\n",
       "         ('a', 'm', 'a'): 431,\n",
       "         ('.', 'j', 'o'): 429,\n",
       "         ('.', 't', 'a'): 424,\n",
       "         ('.', '.', 'f'): 417,\n",
       "         ('.', 'j', 'e'): 403,\n",
       "         ('l', 'e', 'i'): 401,\n",
       "         ('i', 'e', 'l'): 395,\n",
       "         ('.', '.', 'o'): 394,\n",
       "         ('r', 'i', 'e'): 394,\n",
       "         ('.', 'm', 'i'): 393,\n",
       "         ('a', 'n', 'd'): 392,\n",
       "         ('a', 'y', 'a'): 389,\n",
       "         ('.', 'a', 'm'): 384,\n",
       "         ('l', 'e', 'n'): 383,\n",
       "         ('.', 'r', 'o'): 382,\n",
       "         ('y', 'l', 'a'): 381,\n",
       "         ('i', 'n', 'a'): 379,\n",
       "         ('t', 'o', 'n'): 377,\n",
       "         ('.', '.', 'v'): 376,\n",
       "         ('a', 'r', 'a'): 371,\n",
       "         ('.', 'a', 'd'): 366,\n",
       "         ('.', 'l', 'e'): 366,\n",
       "         ('r', 'i', 's'): 360,\n",
       "         ('a', 'm', 'i'): 355,\n",
       "         ('e', 'l', 'y'): 353,\n",
       "         ('a', 'l', 'a'): 353,\n",
       "         ('.', 'c', 'h'): 352,\n",
       "         ('s', 'o', 'n'): 341,\n",
       "         ('l', 'l', 'a'): 337,\n",
       "         ('l', 'l', 'e'): 331,\n",
       "         ('h', 'a', 'r'): 329,\n",
       "         ('d', 'e', 'n'): 318,\n",
       "         ('a', 'l', 'y'): 310,\n",
       "         ('e', 'l', 'a'): 308,\n",
       "         ('.', '.', 'w'): 307,\n",
       "         ('a', 'v', 'i'): 306,\n",
       "         ('y', 'l', 'e'): 303,\n",
       "         ('i', 'g', 'h'): 290,\n",
       "         ('i', 'o', 'n'): 290,\n",
       "         ('.', 'e', 'm'): 288,\n",
       "         ('a', 'r', 'l'): 287,\n",
       "         ('a', 'e', 'l'): 287,\n",
       "         ('l', 'i', 'e'): 285,\n",
       "         ('.', 'm', 'e'): 284,\n",
       "         ('r', 'e', 'n'): 281,\n",
       "         ('i', 'l', 'a'): 279,\n",
       "         ('e', 'n', 'a'): 279,\n",
       "         ('e', 'i', 'g'): 279,\n",
       "         ('a', 's', 'h'): 276,\n",
       "         ('m', 'a', 'n'): 274,\n",
       "         ('l', 'l', 'i'): 271,\n",
       "         ('.', 'l', 'i'): 269,\n",
       "         ('.', 'r', 'e'): 268,\n",
       "         ('e', 'r', 'i'): 268,\n",
       "         ('n', 'n', 'e'): 261,\n",
       "         ('a', 'i', 'l'): 259,\n",
       "         ('m', 'i', 'l'): 259,\n",
       "         ('.', 'c', 'o'): 255,\n",
       "         ('b', 'r', 'i'): 254,\n",
       "         ('r', 'a', 'y'): 254,\n",
       "         ('.', 'b', 'e'): 253,\n",
       "         ('k', 'e', 'n'): 252,\n",
       "         ('.', 'k', 'i'): 250,\n",
       "         ('i', 'r', 'a'): 248,\n",
       "         ('r', 'a', 'n'): 248,\n",
       "         ('.', 'y', 'a'): 246,\n",
       "         ('.', 'a', 'v'): 243,\n",
       "         ('i', 'n', 'e'): 241,\n",
       "         ('e', 'n', 'n'): 240,\n",
       "         ('.', 'k', 'h'): 240,\n",
       "         ('v', 'a', 'n'): 237,\n",
       "         ('c', 'h', 'a'): 236,\n",
       "         ('l', 'a', 'y'): 234,\n",
       "         ('r', 'i', 'n'): 234,\n",
       "         ('.', 'k', 'y'): 229,\n",
       "         ('i', 's', 'h'): 227,\n",
       "         ('a', 'm', 'e'): 226,\n",
       "         ('.', 'n', 'i'): 226,\n",
       "         ('k', 'a', 'i'): 226,\n",
       "         ('l', 'a', 'h'): 224,\n",
       "         ('a', 'n', 'e'): 224,\n",
       "         ('n', 'i', 'e'): 223,\n",
       "         ('r', 'a', 'h'): 222,\n",
       "         ('n', 'n', 'i'): 221,\n",
       "         ('o', 'r', 'i'): 220,\n",
       "         ('e', 'l', 'e'): 219,\n",
       "         ('k', 'a', 'r'): 218,\n",
       "         ('d', 'e', 'l'): 217,\n",
       "         ('k', 'a', 'y'): 215,\n",
       "         ('j', 'a', 'y'): 212,\n",
       "         ('i', 'l', 'l'): 211,\n",
       "         ('m', 'a', 'l'): 210,\n",
       "         ('m', 'e', 'r'): 209,\n",
       "         ('y', 'l', 'i'): 209,\n",
       "         ('.', 't', 'r'): 209,\n",
       "         ('r', 'i', 'c'): 209,\n",
       "         ('l', 'i', 's'): 208,\n",
       "         ('.', 'a', 'a'): 207,\n",
       "         ('d', 'a', 'n'): 207,\n",
       "         ('r', 'e', 'e'): 205,\n",
       "         ('i', 'l', 'y'): 203,\n",
       "         ('e', 't', 't'): 203,\n",
       "         ('d', 'a', 'l'): 203,\n",
       "         ('b', 'r', 'a'): 203,\n",
       "         ('o', 'r', 'a'): 202,\n",
       "         ('b', 'e', 'l'): 201,\n",
       "         ('.', 'r', 'i'): 201,\n",
       "         ('m', 'o', 'n'): 201,\n",
       "         ('e', 'e', 'n'): 200,\n",
       "         ('a', 'r', 'e'): 198,\n",
       "         ('i', 'l', 'e'): 197,\n",
       "         ('e', 'a', 'n'): 196,\n",
       "         ('a', 'l', 'l'): 196,\n",
       "         ('r', 'o', 's'): 196,\n",
       "         ('a', 'r', 'y'): 195,\n",
       "         ('a', 'd', 'e'): 194,\n",
       "         ('.', 'a', 's'): 194,\n",
       "         ('o', 'n', 'n'): 194,\n",
       "         ('c', 'a', 'r'): 193,\n",
       "         ('.', 'k', 'o'): 192,\n",
       "         ('a', 'r', 'r'): 192,\n",
       "         ('.', 'a', 'b'): 190,\n",
       "         ('a', 'd', 'i'): 190,\n",
       "         ('b', 'r', 'e'): 189,\n",
       "         ('a', 'i', 'r'): 189,\n",
       "         ('o', 'n', 'i'): 189,\n",
       "         ('m', 'e', 'l'): 188,\n",
       "         ('.', 'l', 'o'): 186,\n",
       "         ('n', 'a', 'h'): 185,\n",
       "         ('n', 'd', 'r'): 184,\n",
       "         ('.', 's', 'e'): 182,\n",
       "         ('r', 'r', 'i'): 182,\n",
       "         ('.', 't', 'e'): 181,\n",
       "         ('i', 'l', 'i'): 180,\n",
       "         ('e', 'y', 'a'): 179,\n",
       "         ('.', 'j', 'u'): 178,\n",
       "         ('a', 'y', 'd'): 178,\n",
       "         ('l', 'a', 'i'): 177,\n",
       "         ('a', 'n', 't'): 177,\n",
       "         ('a', 'i', 'n'): 177,\n",
       "         ('.', 'p', 'a'): 175,\n",
       "         ('i', 'k', 'a'): 175,\n",
       "         ('a', 'n', 'y'): 175,\n",
       "         ('a', 'a', 'n'): 174,\n",
       "         ('a', 'b', 'e'): 173,\n",
       "         ('l', 'e', 'a'): 173,\n",
       "         ('.', 'a', 'y'): 173,\n",
       "         ('.', 's', 'i'): 172,\n",
       "         ('m', 'i', 'r'): 172,\n",
       "         ('s', 'i', 'a'): 171,\n",
       "         ('k', 'h', 'a'): 171,\n",
       "         ('e', 'r', 'e'): 170,\n",
       "         ('s', 'h', 'i'): 170,\n",
       "         ('.', 'b', 'a'): 169,\n",
       "         ('t', 'h', 'a'): 168,\n",
       "         ('e', 'n', 'i'): 168,\n",
       "         ('.', 'm', 'o'): 168,\n",
       "         ('k', 'a', 'l'): 168,\n",
       "         ('r', 'o', 'n'): 168,\n",
       "         ('a', 'v', 'e'): 166,\n",
       "         ('o', 's', 'e'): 166,\n",
       "         ('s', 's', 'a'): 166,\n",
       "         ('.', 'g', 'r'): 165,\n",
       "         ('s', 't', 'o'): 165,\n",
       "         ('.', 'n', 'e'): 164,\n",
       "         ('a', 'd', 'a'): 164,\n",
       "         ('r', 'e', 'y'): 163,\n",
       "         ('.', 'l', 'u'): 162,\n",
       "         ('d', 'r', 'i'): 162,\n",
       "         ('a', 'v', 'a'): 161,\n",
       "         ('.', 'v', 'i'): 161,\n",
       "         ('e', 'm', 'i'): 160,\n",
       "         ('v', 'e', 'r'): 160,\n",
       "         ('s', 't', 'i'): 159,\n",
       "         ('s', 'a', 'n'): 159,\n",
       "         ('.', 'f', 'a'): 158,\n",
       "         ('a', 's', 'i'): 157,\n",
       "         ('.', 'n', 'o'): 156,\n",
       "         ('i', 's', 't'): 156,\n",
       "         ('.', 'e', 'v'): 154,\n",
       "         ('s', 't', 'e'): 154,\n",
       "         ('.', 'a', 'i'): 154,\n",
       "         ('t', 'a', 'l'): 153,\n",
       "         ('l', 'a', 'r'): 153,\n",
       "         ('m', 'a', 'y'): 153,\n",
       "         ('e', 's', 's'): 153,\n",
       "         ('h', 'a', 'l'): 153,\n",
       "         ('d', 'a', 'r'): 153,\n",
       "         ('h', 'a', 'm'): 153,\n",
       "         ('.', 's', 'o'): 152,\n",
       "         ('.', 'a', 'u'): 152,\n",
       "         ('b', 'e', 'r'): 152,\n",
       "         ('.', 'a', 'z'): 152,\n",
       "         ('e', 'n', 'e'): 151,\n",
       "         ('k', 'a', 'm'): 151,\n",
       "         ('.', 'h', 'e'): 151,\n",
       "         ('j', 'a', 'm'): 151,\n",
       "         ('d', 'e', 'r'): 149,\n",
       "         ('v', 'i', 'n'): 148,\n",
       "         ('v', 'i', 'a'): 147,\n",
       "         ('e', 's', 't'): 147,\n",
       "         ('l', 'o', 'n'): 147,\n",
       "         ('o', 'n', 'a'): 147,\n",
       "         ('r', 'l', 'e'): 146,\n",
       "         ('r', 'i', 'o'): 146,\n",
       "         ('n', 'e', 'l'): 145,\n",
       "         ('a', 'i', 's'): 145,\n",
       "         ('e', 'r', 'a'): 145,\n",
       "         ('n', 'l', 'e'): 143,\n",
       "         ('r', 'e', 'l'): 143,\n",
       "         ('i', 's', 'a'): 142,\n",
       "         ('e', 'v', 'e'): 142,\n",
       "         ('t', 'i', 'n'): 142,\n",
       "         ('a', 't', 'h'): 142,\n",
       "         ('d', 'o', 'n'): 142,\n",
       "         ('s', 'e', 'n'): 142,\n",
       "         ('m', 'i', 'n'): 141,\n",
       "         ('.', 's', 'u'): 141,\n",
       "         ('k', 'e', 'l'): 141,\n",
       "         ('t', 'a', 'n'): 141,\n",
       "         ('n', 'i', 'a'): 141,\n",
       "         ('r', 'y', 'n'): 140,\n",
       "         ('j', 'a', 'n'): 140,\n",
       "         ('n', 'a', 'l'): 140,\n",
       "         ('e', 'r', 'r'): 140,\n",
       "         ('e', 'm', 'a'): 140,\n",
       "         ('s', 't', 'a'): 139,\n",
       "         ('s', 'l', 'e'): 138,\n",
       "         ('a', 'i', 'd'): 138,\n",
       "         ('l', 'o', 'r'): 137,\n",
       "         ('m', 'a', 'd'): 136,\n",
       "         ('s', 'a', 'm'): 136,\n",
       "         ('.', 'g', 'a'): 136,\n",
       "         ('r', 'i', 'y'): 136,\n",
       "         ('.', 's', 't'): 135,\n",
       "         ('i', 'e', 'r'): 135,\n",
       "         ('.', 'z', 'e'): 135,\n",
       "         ('j', 'a', 'h'): 135,\n",
       "         ('k', 'i', 'n'): 134,\n",
       "         ('.', '.', 'x'): 134,\n",
       "         ('l', 'i', 'l'): 133,\n",
       "         ('e', 'v', 'a'): 133,\n",
       "         ('h', 'a', 'y'): 133,\n",
       "         ('t', 'e', 'r'): 133,\n",
       "         ('a', 'y', 's'): 133,\n",
       "         ('.', 't', 'y'): 133,\n",
       "         ('h', 'e', 'r'): 132,\n",
       "         ('r', 'a', 'i'): 132,\n",
       "         ('e', 'o', 'n'): 131,\n",
       "         ('.', 'd', 'i'): 130,\n",
       "         ('c', 'h', 'e'): 130,\n",
       "         ('a', 'c', 'e'): 129,\n",
       "         ('i', 'c', 'k'): 129,\n",
       "         ('.', 'g', 'i'): 128,\n",
       "         ('y', 'o', 'n'): 128,\n",
       "         ('r', 'a', 'e'): 127,\n",
       "         ('n', 'c', 'e'): 127,\n",
       "         ('i', 's', 'e'): 126,\n",
       "         ('.', 'p', 'r'): 125,\n",
       "         ('i', 'n', 'g'): 125,\n",
       "         ('.', 'i', 's'): 124,\n",
       "         ('i', 't', 'h'): 124,\n",
       "         ('l', 'l', 'y'): 124,\n",
       "         ('o', 'l', 'a'): 124,\n",
       "         ('u', 'r', 'i'): 123,\n",
       "         ('a', 'z', 'a'): 123,\n",
       "         ('j', 'e', 'r'): 123,\n",
       "         ('i', 'r', 'e'): 122,\n",
       "         ('i', 's', 's'): 122,\n",
       "         ('t', 't', 'e'): 121,\n",
       "         ('.', 'v', 'a'): 121,\n",
       "         ('c', 'a', 'l'): 121,\n",
       "         ('a', 'k', 'a'): 121,\n",
       "         ('z', 'a', 'r'): 121,\n",
       "         ('y', 'd', 'e'): 121,\n",
       "         ('g', 'r', 'a'): 120,\n",
       "         ('m', 'a', 'i'): 120,\n",
       "         ('h', 'a', 'i'): 119,\n",
       "         ('e', 'n', 'd'): 119,\n",
       "         ('v', 'e', 'n'): 119,\n",
       "         ('.', 'd', 'o'): 119,\n",
       "         ('r', 'i', 't'): 119,\n",
       "         ('v', 'o', 'n'): 119,\n",
       "         ('.', 'l', 'y'): 118,\n",
       "         ('r', 'y', 'a'): 118,\n",
       "         ('l', 'i', 'y'): 117,\n",
       "         ('s', 'e', 'l'): 117,\n",
       "         ('i', 'm', 'a'): 117,\n",
       "         ('b', 'r', 'y'): 116,\n",
       "         ('e', 't', 'h'): 114,\n",
       "         ('d', 'r', 'e'): 113,\n",
       "         ('n', 'a', 'y'): 113,\n",
       "         ('k', 'a', 's'): 113,\n",
       "         ('t', 'e', 'n'): 113,\n",
       "         ('h', 'e', 'l'): 112,\n",
       "         ('o', 'r', 'e'): 112,\n",
       "         ('.', 't', 'o'): 112,\n",
       "         ('a', 't', 'a'): 111,\n",
       "         ('.', 'r', 'y'): 111,\n",
       "         ('e', 'i', 'l'): 111,\n",
       "         ('r', 'a', 'l'): 111,\n",
       "         ('h', 'r', 'i'): 111,\n",
       "         ('a', 'h', 'a'): 111,\n",
       "         ('t', 'o', 'r'): 110,\n",
       "         ('.', 'g', 'e'): 110,\n",
       "         ('a', 'n', 'g'): 110,\n",
       "         ('i', 'd', 'e'): 110,\n",
       "         ('m', 'a', 'k'): 109,\n",
       "         ('.', 't', 'i'): 109,\n",
       "         ('n', 'd', 'e'): 109,\n",
       "         ('a', 'u', 'r'): 108,\n",
       "         ('.', 'r', 'u'): 108,\n",
       "         ('d', 'y', 'n'): 108,\n",
       "         ('n', 'i', 'y'): 108,\n",
       "         ('s', 'h', 'e'): 108,\n",
       "         ('t', 'r', 'i'): 107,\n",
       "         ('n', 'i', 's'): 107,\n",
       "         ('d', 'i', 'e'): 106,\n",
       "         ('t', 'h', 'e'): 106,\n",
       "         ('h', 'e', 'n'): 106,\n",
       "         ('a', 'i', 'a'): 105,\n",
       "         ('j', 'a', 'i'): 105,\n",
       "         ('.', 'o', 'l'): 104,\n",
       "         ('k', 'y', 'l'): 104,\n",
       "         ('n', 'a', 'n'): 104,\n",
       "         ('t', 'r', 'e'): 104,\n",
       "         ('r', 'a', 'c'): 103,\n",
       "         ('a', 'r', 'o'): 103,\n",
       "         ('i', 'n', 'n'): 103,\n",
       "         ('.', 'm', 'y'): 103,\n",
       "         ('s', 's', 'i'): 103,\n",
       "         ('c', 'o', 'r'): 102,\n",
       "         ('a', 's', 't'): 102,\n",
       "         ('a', 'r', 't'): 102,\n",
       "         ('n', 'a', 'i'): 102,\n",
       "         ('n', 'o', 'r'): 101,\n",
       "         ('n', 'a', 't'): 101,\n",
       "         ('e', 'r', 'l'): 101,\n",
       "         ('l', 'e', 's'): 101,\n",
       "         ('a', 'm', 'o'): 101,\n",
       "         ('a', 'h', 'i'): 101,\n",
       "         ('a', 'i', 'y'): 101,\n",
       "         ('o', 'n', 't'): 101,\n",
       "         ('e', 'm', 'm'): 100,\n",
       "         ('t', 'y', 'n'): 100,\n",
       "         ('h', 'a', 'd'): 99,\n",
       "         ('g', 'a', 'n'): 99,\n",
       "         ('.', 't', 'h'): 99,\n",
       "         ('i', 't', 'a'): 99,\n",
       "         ('l', 'y', 'a'): 99,\n",
       "         ('a', 'n', 's'): 99,\n",
       "         ('a', 'k', 'i'): 99,\n",
       "         ('a', 'n', 'o'): 98,\n",
       "         ('d', 'i', 'a'): 98,\n",
       "         ('m', 'o', 'r'): 98,\n",
       "         ('a', 'y', 'n'): 98,\n",
       "         ('a', 'k', 'e'): 98,\n",
       "         ('m', 'a', 't'): 98,\n",
       "         ('y', 'i', 'a'): 98,\n",
       "         ('r', 'l', 'i'): 97,\n",
       "         ('y', 'e', 'l'): 97,\n",
       "         ('z', 'a', 'y'): 97,\n",
       "         ('r', 'a', 'm'): 97,\n",
       "         ('v', 'a', 'l'): 96,\n",
       "         ('u', 'l', 'i'): 96,\n",
       "         ('y', 'r', 'i'): 96,\n",
       "         ('a', 's', 's'): 96,\n",
       "         ('i', 'd', 'a'): 96,\n",
       "         ('h', 'o', 'n'): 96,\n",
       "         ('.', 'k', 'r'): 96,\n",
       "         ('m', 'i', 'a'): 95,\n",
       "         ('e', 'n', 't'): 95,\n",
       "         ('v', 'i', 'e'): 95,\n",
       "         ('a', 'd', 'd'): 94,\n",
       "         ('c', 'h', 'i'): 94,\n",
       "         ('i', 'z', 'a'): 93,\n",
       "         ('n', 'e', 's'): 93,\n",
       "         ('j', 'o', 's'): 93,\n",
       "         ('k', 'a', 't'): 93,\n",
       "         ('.', 'e', 's'): 93,\n",
       "         ('y', 'n', 'e'): 93,\n",
       "         ('a', 'z', 'i'): 93,\n",
       "         ('c', 'a', 'm'): 92,\n",
       "         ('.', 'p', 'e'): 92,\n",
       "         ('e', 'a', 'h'): 92,\n",
       "         ('.', '.', 'q'): 92,\n",
       "         ('c', 'e', 'l'): 92,\n",
       "         ('.', 'n', 'y'): 92,\n",
       "         ('a', 'r', 's'): 92,\n",
       "         ('a', 'r', 'd'): 92,\n",
       "         ('l', 'e', 't'): 91,\n",
       "         ('.', 'w', 'i'): 91,\n",
       "         ('i', 'e', 'n'): 91,\n",
       "         ('a', 'm', 'y'): 91,\n",
       "         ('.', 'z', 'y'): 91,\n",
       "         ('d', 'a', 'm'): 91,\n",
       "         ('.', 'a', 'h'): 91,\n",
       "         ('i', 'u', 's'): 91,\n",
       "         ('o', 'v', 'a'): 90,\n",
       "         ('s', 'a', 'l'): 90,\n",
       "         ('n', 'i', 'c'): 90,\n",
       "         ('y', 'n', 'a'): 90,\n",
       "         ('n', 'd', 'a'): 90,\n",
       "         ('.', 'e', 'r'): 90,\n",
       "         ('d', 'a', 'y'): 90,\n",
       "         ('.', 'r', 'h'): 90,\n",
       "         ('v', 'i', 'o'): 89,\n",
       "         ('s', 'y', 'n'): 89,\n",
       "         ('n', 'a', 's'): 89,\n",
       "         ('m', 'a', 'h'): 89,\n",
       "         ('a', 'r', 'm'): 88,\n",
       "         ('a', 'h', 'm'): 88,\n",
       "         ('r', 'e', 't'): 87,\n",
       "         ('l', 'e', 'r'): 87,\n",
       "         ('i', 'n', 'i'): 87,\n",
       "         ('j', 'e', 'n'): 87,\n",
       "         ('i', 'n', 'd'): 87,\n",
       "         ('g', 'e', 'n'): 86,\n",
       "         ('d', 'r', 'a'): 86,\n",
       "         ('r', 'e', 's'): 86,\n",
       "         ('n', 'g', 'e'): 86,\n",
       "         ('o', 'n', 'e'): 86,\n",
       "         ('.', 'z', 'i'): 86,\n",
       "         ('.', 'y', 'u'): 86,\n",
       "         ('.', 'y', 'o'): 86,\n",
       "         ('a', 'b', 'r'): 85,\n",
       "         ('g', 'e', 'l'): 85,\n",
       "         ('.', 'b', 'l'): 85,\n",
       "         ('d', 'e', 'm'): 85,\n",
       "         ('e', 'y', 'l'): 85,\n",
       "         ('.', 'm', 'u'): 85,\n",
       "         ('e', 'r', 'y'): 84,\n",
       "         ('l', 'y', 's'): 84,\n",
       "         ('h', 'i', 'r'): 84,\n",
       "         ('b', 'e', 'n'): 84,\n",
       "         ('o', 'l', 'u'): 84,\n",
       "         ('u', 'w', 'a'): 84,\n",
       "         ('e', 'l', 'o'): 83,\n",
       "         ('z', 'e', 'l'): 83,\n",
       "         ('o', 'l', 'e'): 83,\n",
       "         ('s', 'a', 'r'): 83,\n",
       "         ('l', 'e', 'x'): 83,\n",
       "         ('e', 'v', 'i'): 83,\n",
       "         ('a', 'n', 'c'): 83,\n",
       "         ('a', 'a', 'r'): 83,\n",
       "         ('e', 's', 'h'): 83,\n",
       "         ('.', 'q', 'u'): 82,\n",
       "         ('j', 'a', 's'): 82,\n",
       "         ('m', 'i', 'k'): 82,\n",
       "         ('r', 'e', 'i'): 82,\n",
       "         ('a', 's', 'e'): 82,\n",
       "         ('y', 'l', 'y'): 82,\n",
       "         ('n', 't', 'a'): 82,\n",
       "         ('h', 'i', 'a'): 81,\n",
       "         ('l', 'i', 'z'): 81,\n",
       "         ('t', 'a', 'y'): 81,\n",
       "         ('m', 'a', 'e'): 81,\n",
       "         ('j', 'a', 'c'): 81,\n",
       "         ('a', 'l', 'o'): 81,\n",
       "         ('h', 'i', 'l'): 81,\n",
       "         ('n', 'i', 'k'): 81,\n",
       "         ('n', 'n', 'y'): 81,\n",
       "         ('s', 's', 'e'): 81,\n",
       "         ('r', 'l', 'y'): 80,\n",
       "         ('e', 'm', 'e'): 80,\n",
       "         ('.', 'e', 'd'): 80,\n",
       "         ('n', 'e', 'e'): 80,\n",
       "         ('r', 'r', 'a'): 80,\n",
       "         ('m', 'i', 'e'): 80,\n",
       "         ('n', 'y', 'a'): 80,\n",
       "         ('o', 'u', 'r'): 79,\n",
       "         ('.', 'f', 'r'): 79,\n",
       "         ('a', 'y', 'v'): 79,\n",
       "         ('e', 'e', 'l'): 79,\n",
       "         ('l', 'u', 'w'): 79,\n",
       "         ('i', 'v', 'i'): 78,\n",
       "         ('z', 'i', 'e'): 78,\n",
       "         ('c', 'o', 'l'): 78,\n",
       "         ('y', 'r', 'a'): 78,\n",
       "         ('l', 'i', 'o'): 78,\n",
       "         ('.', '.', 'u'): 78,\n",
       "         ('p', 'e', 'r'): 77,\n",
       "         ('m', 'a', 'c'): 77,\n",
       "         ('o', 'r', 'd'): 77,\n",
       "         ('a', 'c', 'h'): 77,\n",
       "         ('.', 'h', 'o'): 77,\n",
       "         ('n', 't', 'e'): 77,\n",
       "         ('y', 'a', 'r'): 77,\n",
       "         ('j', 'a', 'l'): 77,\n",
       "         ('.', 'd', 'r'): 77,\n",
       "         ('.', 'b', 'o'): 77,\n",
       "         ('y', 'c', 'e'): 77,\n",
       "         ('s', 'a', 'b'): 76,\n",
       "         ('v', 'e', 'l'): 76,\n",
       "         ('a', 'b', 'i'): 76,\n",
       "         ('e', 'r', 's'): 76,\n",
       "         ('n', 'e', 'y'): 76,\n",
       "         ('i', 'c', 'h'): 76,\n",
       "         ('c', 'a', 's'): 76,\n",
       "         ('a', 'y', 'e'): 76,\n",
       "         ('u', 's', 't'): 76,\n",
       "         ('n', 'a', 'v'): 76,\n",
       "         ('d', 'i', 'n'): 76,\n",
       "         ('t', 'r', 'a'): 76,\n",
       "         ('e', 'e', 'm'): 76,\n",
       "         ('u', 'a', 'n'): 76,\n",
       "         ('r', 'y', 'l'): 75,\n",
       "         ('i', 'r', 'i'): 75,\n",
       "         ('m', 'i', 'y'): 75,\n",
       "         ('u', 'e', 'l'): 75,\n",
       "         ('k', 'e', 'i'): 75,\n",
       "         ('k', 'y', 'n'): 75,\n",
       "         ('t', 'i', 'a'): 75,\n",
       "         ('.', 'a', 'k'): 75,\n",
       "         ('o', 'm', 'i'): 74,\n",
       "         ('j', 'u', 'l'): 74,\n",
       "         ('s', 'i', 'e'): 74,\n",
       "         ('a', 't', 'i'): 74,\n",
       "         ('a', 'h', 'l'): 73,\n",
       "         ('.', 'j', 'i'): 73,\n",
       "         ('a', 't', 't'): 73,\n",
       "         ('z', 'a', 'n'): 73,\n",
       "         ('e', 'e', 'r'): 73,\n",
       "         ('m', 'm', 'a'): 72,\n",
       "         ('.', 'z', 'o'): 72,\n",
       "         ('r', 'e', 'a'): 72,\n",
       "         ('.', 'a', 't'): 72,\n",
       "         ('y', 'l', 'o'): 72,\n",
       "         ('t', 'h', 'i'): 72,\n",
       "         ('k', 'e', 'y'): 72,\n",
       "         ('n', 'a', 'r'): 72,\n",
       "         ('r', 'r', 'e'): 72,\n",
       "         ('.', 'f', 'i'): 71,\n",
       "         ('.', 's', 'y'): 71,\n",
       "         ('c', 'i', 'a'): 71,\n",
       "         ('d', 'e', 's'): 71,\n",
       "         ('o', 'h', 'a'): 71,\n",
       "         ('s', 'a', 'i'): 71,\n",
       "         ('t', 'a', 'r'): 71,\n",
       "         ('z', 'i', 'a'): 71,\n",
       "         ('i', 'n', 'o'): 71,\n",
       "         ('o', 'l', 'l'): 70,\n",
       "         ('q', 'u', 'e'): 70,\n",
       "         ('i', 'v', 'a'): 70,\n",
       "         ('a', 'r', 'c'): 70,\n",
       "         ('k', 'a', 'n'): 70,\n",
       "         ('o', 'l', 'i'): 69,\n",
       "         ('b', 'r', 'o'): 69,\n",
       "         ('l', 'u', 'c'): 69,\n",
       "         ('k', 'e', 'r'): 69,\n",
       "         ('d', 'a', 'i'): 69,\n",
       "         ('r', 'e', 'm'): 69,\n",
       "         ('a', 'd', 'r'): 69,\n",
       "         ('c', 'h', 'r'): 69,\n",
       "         ('d', 'a', 'v'): 69,\n",
       "         ('z', 'a', 'i'): 69,\n",
       "         ('l', 'a', 'm'): 69,\n",
       "         ('y', 'a', 's'): 69,\n",
       "         ('m', 'e', 'e'): 69,\n",
       "         ('i', 's', 'o'): 68,\n",
       "         ('.', 'c', 'l'): 68,\n",
       "         ('e', 'n', 'z'): 68,\n",
       "         ('r', 'y', 's'): 68,\n",
       "         ('.', 'w', 'a'): 68,\n",
       "         ('s', 'i', 'r'): 68,\n",
       "         ('i', 'h', 'a'): 68,\n",
       "         ('o', 'u', 's'): 68,\n",
       "         ('s', 'e', 'r'): 67,\n",
       "         ('q', 'u', 'i'): 67,\n",
       "         ('.', 'v', 'e'): 67,\n",
       "         ('m', 'i', 'c'): 67,\n",
       "         ('.', 'c', 'r'): 67,\n",
       "         ('.', 'h', 'u'): 67,\n",
       "         ('.', 'i', 'l'): 67,\n",
       "         ('s', 't', 'y'): 67,\n",
       "         ('a', 'i', 'z'): 67,\n",
       "         ('l', 'a', 'k'): 66,\n",
       "         ('a', 't', 'e'): 66,\n",
       "         ('z', 'a', 'l'): 66,\n",
       "         ('o', 'm', 'a'): 66,\n",
       "         ('h', 'a', 'w'): 66,\n",
       "         ('a', 'u', 'n'): 66,\n",
       "         ('.', 'c', 'e'): 65,\n",
       "         ('i', 'l', 'o'): 65,\n",
       "         ('a', 'b', 'd'): 65,\n",
       "         ('d', 'i', 'l'): 64,\n",
       "         ('e', 'r', 'o'): 64,\n",
       "         ('k', 'r', 'i'): 64,\n",
       "         ('o', 'r', 'r'): 64,\n",
       "         ('t', 'e', 'l'): 63,\n",
       "         ('i', 's', 'l'): 63,\n",
       "         ('n', 's', 'l'): 63,\n",
       "         ('i', 'c', 'e'): 63,\n",
       "         ('m', 'y', 'a'): 63,\n",
       "         ('o', 's', 'a'): 63,\n",
       "         ('n', 'e', 't'): 63,\n",
       "         ('k', 'e', 'e'): 63,\n",
       "         ('k', 'o', 'l'): 63,\n",
       "         ('d', 'e', 'e'): 63,\n",
       "         ('r', 'i', 'l'): 62,\n",
       "         ('o', 'r', 'y'): 62,\n",
       "         ('k', 'o', 'r'): 62,\n",
       "         ('j', 'a', 'k'): 62,\n",
       "         ('h', 'a', 's'): 62,\n",
       "         ('h', 'a', 'a'): 62,\n",
       "         ('i', 'm', 'i'): 62,\n",
       "         ('p', 'h', 'i'): 61,\n",
       "         ('b', 'e', 't'): 61,\n",
       "         ('u', 'n', 'a'): 61,\n",
       "         ('a', 'r', 'k'): 61,\n",
       "         ('e', 's', 'l'): 61,\n",
       "         ('a', 'y', 't'): 61,\n",
       "         ('i', 'v', 'e'): 61,\n",
       "         ('j', 'e', 's'): 61,\n",
       "         ('e', 'n', 's'): 61,\n",
       "         ('e', 'n', 'l'): 61,\n",
       "         ('k', 'a', 'e'): 61,\n",
       "         ('.', 'w', 'e'): 61,\n",
       "         ('.', 'y', 'e'): 61,\n",
       "         ('a', 'z', 'e'): 60,\n",
       "         ('c', 'l', 'a'): 60,\n",
       "         ('m', 'e', 'n'): 60,\n",
       "         ('l', 'e', 'o'): 60,\n",
       "         ('.', 'e', 'n'): 60,\n",
       "         ('n', 'd', 'i'): 60,\n",
       "         ('r', 'i', 'k'): 60,\n",
       "         ('.', 'i', 'n'): 60,\n",
       "         ('t', 'l', 'e'): 60,\n",
       "         ('s', 'i', 'n'): 60,\n",
       "         ('g', 'e', 'r'): 60,\n",
       "         ('a', 's', 'a'): 60,\n",
       "         ('a', 'c', 'k'): 59,\n",
       "         ('m', 'b', 'e'): 59,\n",
       "         ('c', 'i', 'e'): 59,\n",
       "         ('.', 'i', 'z'): 59,\n",
       "         ('a', 'd', 'o'): 59,\n",
       "         ('d', 'o', 'r'): 59,\n",
       "         ('y', 's', 'e'): 59,\n",
       "         ('s', 'l', 'y'): 59,\n",
       "         ('z', 'l', 'e'): 59,\n",
       "         ('i', 'j', 'a'): 59,\n",
       "         ('n', 'e', 'r'): 59,\n",
       "         ('a', 'v', 'o'): 59,\n",
       "         ('l', 'l', 'o'): 58,\n",
       "         ('i', 'n', 's'): 58,\n",
       "         ('n', 'i', 't'): 58,\n",
       "         ('e', 'n', 'c'): 58,\n",
       "         ('y', 's', 'o'): 58,\n",
       "         ('g', 'h', 'a'): 58,\n",
       "         ('a', 'y', 'c'): 58,\n",
       "         ('i', 'z', 'e'): 58,\n",
       "         ('v', 'i', 'k'): 58,\n",
       "         ('e', 's', 'i'): 57,\n",
       "         ('h', 'i', 'n'): 57,\n",
       "         ('e', 'i', 'a'): 57,\n",
       "         ('i', 'c', 'a'): 57,\n",
       "         ('r', 'o', 'm'): 57,\n",
       "         ('r', 'm', 'a'): 57,\n",
       "         ('h', 'y', 'a'): 57,\n",
       "         ('a', 'd', 'y'): 57,\n",
       "         ('a', 'h', 'n'): 57,\n",
       "         ('.', 'x', 'a'): 57,\n",
       "         ('w', 'i', 'l'): 56,\n",
       "         ('e', 'd', 'e'): 56,\n",
       "         ('a', 'c', 'i'): 56,\n",
       "         ('u', 'r', 'a'): 56,\n",
       "         ('h', 'l', 'a'): 56,\n",
       "         ('j', 'a', 'z'): 56,\n",
       "         ('c', 'a', 'i'): 56,\n",
       "         ('d', 'e', 'v'): 56,\n",
       "         ('v', 'a', 'r'): 56,\n",
       "         ('q', 'u', 'a'): 56,\n",
       "         ('.', 's', 'k'): 55,\n",
       "         ('k', 'i', 'a'): 55,\n",
       "         ('w', 'i', 'n'): 55,\n",
       "         ('t', 'a', 'v'): 55,\n",
       "         ('j', 'o', 'h'): 55,\n",
       "         ('l', 'a', 's'): 55,\n",
       "         ('e', 'd', 'a'): 55,\n",
       "         ('f', 'a', 'r'): 55,\n",
       "         ('z', 'e', 'n'): 55,\n",
       "         ('.', 'a', 'e'): 55,\n",
       "         ('.', 'h', 'i'): 55,\n",
       "         ('a', 'y', 'm'): 55,\n",
       "         ('t', 'a', 'i'): 55,\n",
       "         ('i', 'a', 's'): 55,\n",
       "         ('l', 'i', 'v'): 54,\n",
       "         ('f', 'i', 'n'): 54,\n",
       "         ('j', 'o', 'r'): 54,\n",
       "         ('r', 'n', 'e'): 54,\n",
       "         ('b', 'l', 'a'): 54,\n",
       "         ('i', 'a', 'r'): 54,\n",
       "         ('a', 'l', 'd'): 54,\n",
       "         ('a', 'm', 'r'): 54,\n",
       "         ('f', 'r', 'a'): 54,\n",
       "         ('n', 't', 'o'): 54,\n",
       "         ('.', 'e', 'i'): 54,\n",
       "         ('n', 's', 'h'): 54,\n",
       "         ('y', 'r', 'e'): 54,\n",
       "         ('n', 'd', 'y'): 53,\n",
       "         ('h', 'a', 'v'): 53,\n",
       "         ('k', 'a', 'h'): 53,\n",
       "         ('t', 't', 'a'): 53,\n",
       "         ('s', 'o', 'l'): 53,\n",
       "         ('r', 'i', 'g'): 53,\n",
       "         ('a', 'e', 'd'): 53,\n",
       "         ('n', 'o', 'v'): 52,\n",
       "         ('c', 'k', 'e'): 52,\n",
       "         ('e', 'g', 'a'): 52,\n",
       "         ('i', 'a', 'm'): 52,\n",
       "         ('.', 'p', 'h'): 52,\n",
       "         ('e', 's', 'a'): 52,\n",
       "         ('.', 'e', 'z'): 52,\n",
       "         ('h', 'y', 'l'): 52,\n",
       "         ('a', 'k', 'y'): 52,\n",
       "         ('a', 'w', 'n'): 52,\n",
       "         ('a', 'j', 'a'): 52,\n",
       "         ('i', 'k', 'o'): 52,\n",
       "         ('d', 'd', 'i'): 51,\n",
       "         ('g', 'a', 'r'): 51,\n",
       "         ('n', 'd', 'o'): 51,\n",
       "         ('e', 'i', 'r'): 51,\n",
       "         ('s', 'e', 'y'): 51,\n",
       "         ('t', 'i', 'e'): 51,\n",
       "         ('a', 'm', 'b'): 51,\n",
       "         ('p', 'r', 'i'): 51,\n",
       "         ('e', 'r', 't'): 51,\n",
       "         ('r', 'i', 'd'): 51,\n",
       "         ('k', 'i', 'e'): 51,\n",
       "         ('j', 'a', 'e'): 51,\n",
       "         ('i', 't', 't'): 51,\n",
       "         ('h', 'i', 't'): 51,\n",
       "         ('a', 'i', 'm'): 51,\n",
       "         ('r', 'a', 'd'): 51,\n",
       "         ('a', 'y', 'o'): 51,\n",
       "         ('e', 'a', 'l'): 51,\n",
       "         ('g', 'i', 'a'): 50,\n",
       "         ('e', 'p', 'h'): 50,\n",
       "         ('a', 'i', 't'): 50,\n",
       "         ('z', 'a', 'h'): 50,\n",
       "         ('c', 'a', 'y'): 50,\n",
       "         ('a', 'n', 'u'): 50,\n",
       "         ('r', 'i', 'u'): 50,\n",
       "         ('a', 'u', 'd'): 49,\n",
       "         ('.', 'i', 'v'): 49,\n",
       "         ('.', 'i', 'r'): 49,\n",
       "         ('e', 's', 'e'): 49,\n",
       "         ('m', 'y', 'l'): 49,\n",
       "         ('.', 'f', 'e'): 49,\n",
       "         ('i', 'd', 'y'): 49,\n",
       "         ('y', 'a', 'l'): 49,\n",
       "         ('r', 'e', 'd'): 49,\n",
       "         ('h', 'a', 'e'): 49,\n",
       "         ('g', 'r', 'e'): 49,\n",
       "         ('k', 'y', 'r'): 49,\n",
       "         ('b', 'a', 'r'): 49,\n",
       "         ('a', 'n', 'v'): 49,\n",
       "         ('e', 'r', 'm'): 49,\n",
       "         ('m', 'e', 's'): 49,\n",
       "         ('j', 'a', 'r'): 49,\n",
       "         ('a', 'r', 'v'): 49,\n",
       "         ('n', 't', 'i'): 48,\n",
       "         ('u', 'r', 'e'): 48,\n",
       "         ('.', 'm', 'c'): 48,\n",
       "         ('a', 'k', 'o'): 48,\n",
       "         ('w', 'y', 'n'): 48,\n",
       "         ('u', 'l', 'a'): 48,\n",
       "         ('m', 'e', 'i'): 48,\n",
       "         ('n', 'a', 'e'): 48,\n",
       "         ('o', 'm', 'e'): 48,\n",
       "         ('m', 'a', 's'): 48,\n",
       "         ('r', 'a', 's'): 48,\n",
       "         ('j', 'a', 'v'): 48,\n",
       "         ('u', 'i', 'n'): 47,\n",
       "         ('o', 'e', 'l'): 47,\n",
       "         ('.', 'z', 'u'): 47,\n",
       "         ('w', 'e', 'n'): 47,\n",
       "         ('n', 'o', 'n'): 47,\n",
       "         ('e', 'd', 'i'): 47,\n",
       "         ('i', 'n', 'c'): 47,\n",
       "         ('a', 'a', 'd'): 47,\n",
       "         ('e', 'i', 's'): 47,\n",
       "         ('c', 'e', 'n'): 47,\n",
       "         ('.', 'o', 'r'): 47,\n",
       "         ('z', 'a', 'm'): 47,\n",
       "         ('z', 'a', 'k'): 47,\n",
       "         ('i', 'r', 'o'): 47,\n",
       "         ('a', 'a', 'l'): 46,\n",
       "         ('n', 'z', 'i'): 46,\n",
       "         ('e', 'e', 's'): 46,\n",
       "         ('l', 'a', 'u'): 46,\n",
       "         ('o', 'n', 'd'): 46,\n",
       "         ('n', 'y', 'l'): 46,\n",
       "         ('s', 'e', 'a'): 46,\n",
       "         ('e', 'm', 'o'): 46,\n",
       "         ('.', 'c', 'y'): 46,\n",
       "         ('s', 'i', 'm'): 46,\n",
       "         ('m', 'i', 's'): 46,\n",
       "         ('a', 'v', 'y'): 46,\n",
       "         ('u', 'm', 'a'): 46,\n",
       "         ('k', 'h', 'y'): 46,\n",
       "         ('r', 'u', 's'): 46,\n",
       "         ('a', 'h', 'e'): 46,\n",
       "         ('r', 's', 'h'): 46,\n",
       "         ('y', 'a', 'a'): 46,\n",
       "         ('p', 'a', 'r'): 45,\n",
       "         ('h', 'e', 'a'): 45,\n",
       "         ('f', 'e', 'r'): 45,\n",
       "         ('y', 's', 'i'): 45,\n",
       "         ('y', 'd', 'a'): 45,\n",
       "         ('m', 'a', 'x'): 45,\n",
       "         ('.', 'g', 'u'): 45,\n",
       "         ('n', 'v', 'i'): 45,\n",
       "         ('t', 'a', 'm'): 45,\n",
       "         ('a', 't', 'o'): 45,\n",
       "         ('j', 'o', 'n'): 45,\n",
       "         ('r', 'l', 'o'): 44,\n",
       "         ('s', 'k', 'y'): 44,\n",
       "         ('n', 't', 'h'): 44,\n",
       "         ('i', 't', 'y'): 44,\n",
       "         ('j', 'a', 'd'): 44,\n",
       "         ('i', 'n', 'l'): 44,\n",
       "         ('k', 'i', 'm'): 44,\n",
       "         ('o', 's', 'i'): 44,\n",
       "         ('f', 'r', 'e'): 44,\n",
       "         ('k', 'l', 'e'): 44,\n",
       "         ('i', 't', 'z'): 44,\n",
       "         ('s', 'a', 'h'): 44,\n",
       "         ('.', 'c', 'i'): 44,\n",
       "         ('e', 'n', 'o'): 44,\n",
       "         ('d', 'e', 'a'): 44,\n",
       "         ('a', 'y', 'r'): 44,\n",
       "         ('e', 'v', 'o'): 44,\n",
       "         ('c', 'o', 'n'): 44,\n",
       "         ('s', 'h', 'o'): 44,\n",
       "         ('h', 'm', 'a'): 44,\n",
       "         ('e', 'c', 'k'): 44,\n",
       "         ('j', 'a', 'x'): 44,\n",
       "         ('b', 'a', 's'): 44,\n",
       "         ('d', 'h', 'a'): 44,\n",
       "         ('o', 'n', 'y'): 43,\n",
       "         ('s', 'i', 'd'): 43,\n",
       "         ('y', 's', 't'): 43,\n",
       "         ('l', 'o', 'u'): 43,\n",
       "         ('e', 'n', 'y'): 43,\n",
       "         ('y', 'v', 'i'): 43,\n",
       "         ('s', 'm', 'a'): 43,\n",
       "         ('u', 'h', 'a'): 43,\n",
       "         ('v', 'e', 'e'): 43,\n",
       "         ('m', 'i', 't'): 43,\n",
       "         ('t', 'h', 'o'): 43,\n",
       "         ('y', 'm', 'a'): 43,\n",
       "         ('d', 'i', 's'): 42,\n",
       "         ('h', 'a', 'z'): 42,\n",
       "         ('r', 'o', 'l'): 42,\n",
       "         ('a', 'g', 'a'): 42,\n",
       "         ('o', 'a', 'n'): 42,\n",
       "         ('e', 'l', 's'): 42,\n",
       "         ('s', 'a', 'y'): 42,\n",
       "         ('k', 'i', 'r'): 42,\n",
       "         ('i', 'n', 't'): 42,\n",
       "         ('n', 'n', 'o'): 42,\n",
       "         ('a', 'q', 'u'): 42,\n",
       "         ('s', 'a', 'a'): 42,\n",
       "         ('v', 'y', 'n'): 42,\n",
       "         ('s', 'h', 'r'): 42,\n",
       "         ('h', 'i', 'k'): 42,\n",
       "         ('y', 's', 'h'): 42,\n",
       "         ('a', 'm', 'm'): 42,\n",
       "         ('.', 'o', 's'): 42,\n",
       "         ('k', 'l', 'y'): 41,\n",
       "         ('y', 'd', 'i'): 41,\n",
       "         ('s', 'h', 'l'): 41,\n",
       "         ('t', 'l', 'y'): 41,\n",
       "         ('.', 'b', 'i'): 41,\n",
       "         ('h', 'l', 'i'): 41,\n",
       "         ('r', 'a', 'v'): 41,\n",
       "         ('e', 'r', 'n'): 41,\n",
       "         ('v', 'i', 's'): 41,\n",
       "         ('k', 'a', 'd'): 41,\n",
       "         ('k', 's', 'h'): 41,\n",
       "         ('.', 'j', 'h'): 41,\n",
       "         ('h', 'n', 'a'): 41,\n",
       "         ('r', 'y', 'e'): 41,\n",
       "         ('k', 'h', 'i'): 41,\n",
       "         ('z', 'a', 'b'): 40,\n",
       "         ('n', 'e', 'v'): 40,\n",
       "         ('d', 'l', 'e'): 40,\n",
       "         ('r', 's', 'o'): 40,\n",
       "         ('y', 's', 's'): 40,\n",
       "         ('c', 'a', 't'): 40,\n",
       "         ('p', 'h', 'a'): 40,\n",
       "         ('z', 'l', 'y'): 40,\n",
       "         ('h', 'o', 'l'): 40,\n",
       "         ('y', 'a', 'm'): 40,\n",
       "         ('o', 'd', 'i'): 40,\n",
       "         ('r', 'a', 't'): 40,\n",
       "         ('r', 'h', 'y'): 40,\n",
       "         ('a', 'r', 'n'): 40,\n",
       "         ('a', 'k', 's'): 40,\n",
       "         ('r', 'r', 'o'): 40,\n",
       "         ('r', 'a', 'j'): 40,\n",
       "         ('a', 'x', 't'): 40,\n",
       "         ('n', 'a', 'm'): 40,\n",
       "         ('a', 'y', 'z'): 40,\n",
       "         ('m', 'a', 'a'): 40,\n",
       "         ('e', 'h', 'a'): 40,\n",
       "         ('o', 'h', 'n'): 40,\n",
       "         ('h', 'e', 'e'): 40,\n",
       "         ('w', 'a', 'n'): 39,\n",
       "         ('n', 'i', 'n'): 39,\n",
       "         ('i', 'c', 'i'): 39,\n",
       "         ('l', 'i', 't'): 39,\n",
       "         ('m', 'b', 'r'): 39,\n",
       "         ('o', 'v', 'e'): 39,\n",
       "         ('k', 'i', 'y'): 39,\n",
       "         ('c', 'a', 'n'): 39,\n",
       "         ('h', 'i', 'm'): 39,\n",
       "         ('l', 'd', 'e'): 39,\n",
       "         ('p', 'r', 'a'): 39,\n",
       "         ('k', 'e', 'a'): 39,\n",
       "         ('r', 'o', 'o'): 38,\n",
       "         ('y', 't', 'o'): 38,\n",
       "         ('x', 'a', 'n'): 38,\n",
       "         ('p', 'r', 'e'): 38,\n",
       "         ('y', 'e', 'r'): 38,\n",
       "         ('n', 'a', 'd'): 38,\n",
       "         ('t', 't', 'i'): 38,\n",
       "         ('a', 't', 'r'): 38,\n",
       "         ('d', 'a', 's'): 38,\n",
       "         ('b', 'i', 'n'): 38,\n",
       "         ('n', 's', 'e'): 38,\n",
       "         ('d', 'a', 'e'): 38,\n",
       "         ('d', 'o', 'm'): 38,\n",
       "         ('l', 'a', 'v'): 38,\n",
       "         ('a', 'c', 'a'): 38,\n",
       "         ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27,27,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "s_to_i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "s_to_i[\".\"] = 0\n",
    "i_to_s = {i: s for s, i in s_to_i.items()}\n",
    "i_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chars = [\".\"]*2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chars, chars[1:], chars[2:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        ix3 = s_to_i[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.imshow(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "# plt.imshow(N, cmap=\"Blues\")\n",
    "\n",
    "# for i in range(27):\n",
    "#     for j in range(27):\n",
    "#         for k in range(27):\n",
    "#             chstr = i_to_s[i] + i_to_s[j] + i_to_s[k]\n",
    "#             plt.text(k, j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "#             plt.text(k, j, i, N[i,j,k].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "\n",
    "# plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N[0][0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(seed)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "i_to_s[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   0, 4410, 1306,  ...,  134,  535,  929],\n",
       "         [   0,  207,  190,  ...,   27,  173,  152],\n",
       "         [   0,  169,    0,  ...,    0,    4,    0],\n",
       "         ...,\n",
       "         [   0,   57,    0,  ...,    1,   17,   11],\n",
       "         [   0,  246,    0,  ...,    0,    0,    2],\n",
       "         [   0,  456,    0,  ...,    0,   91,    1]],\n",
       "\n",
       "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [  40,    0,    5,  ...,    0,   20,   11],\n",
       "         [  36,   28,   20,  ...,    0,   12,    0],\n",
       "         ...,\n",
       "         [  11,    5,    0,  ...,   17,    6,    3],\n",
       "         [ 163,  389,   13,  ...,    0,   16,   40],\n",
       "         [  38,  123,    0,  ...,    0,   12,   22]],\n",
       "\n",
       "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [  46,    5,    5,  ...,    4,   31,    4],\n",
       "         [   1,    8,    0,  ...,    0,    9,    0],\n",
       "         ...,\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [  55,    4,    1,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [  10,    0,    2,  ...,    0,   10,    0],\n",
       "         [   0,    0,    0,  ...,    0,    1,    0],\n",
       "         ...,\n",
       "         [  18,    3,    0,  ...,    0,    1,    0],\n",
       "         [   5,    4,    0,  ...,    0,    0,    0],\n",
       "         [   0,   16,    0,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [ 716,   46,   10,  ...,    3,    6,   21],\n",
       "         [   2,    2,    0,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [  23,    1,    0,  ...,    1,    0,    0],\n",
       "         [   1,   18,    0,  ...,    0,    0,    0],\n",
       "         [   2,   27,    0,  ...,    1,    0,    0]],\n",
       "\n",
       "        [[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [  98,   14,   40,  ...,    3,   97,    3],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   1,    0,    0,  ...,    0,    0,    0],\n",
       "         [  34,   27,    0,  ...,    0,    0,    1],\n",
       "         [   4,   13,    0,  ...,    0,    7,    0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.1192e-05, 1.3759e-01, 4.0767e-02,  ..., 4.2109e-03,\n",
       "          1.6719e-02, 2.9008e-02],\n",
       "         [2.2538e-04, 4.6879e-02, 4.3047e-02,  ..., 6.3106e-03,\n",
       "          3.9216e-02, 3.4483e-02],\n",
       "         [7.5019e-04, 1.2753e-01, 7.5019e-04,  ..., 7.5019e-04,\n",
       "          3.7509e-03, 7.5019e-04],\n",
       "         ...,\n",
       "         [6.2112e-03, 3.6025e-01, 6.2112e-03,  ..., 1.2422e-02,\n",
       "          1.1180e-01, 7.4534e-02],\n",
       "         [1.7794e-03, 4.3950e-01, 1.7794e-03,  ..., 1.7794e-03,\n",
       "          1.7794e-03, 5.3381e-03],\n",
       "         [1.0460e-03, 4.7803e-01, 1.0460e-03,  ..., 1.0460e-03,\n",
       "          9.6234e-02, 2.0921e-03]],\n",
       "\n",
       "        [[3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02],\n",
       "         [7.0326e-02, 1.7153e-03, 1.0292e-02,  ..., 1.7153e-03,\n",
       "          3.6021e-02, 2.0583e-02],\n",
       "         [6.5141e-02, 5.1056e-02, 3.6972e-02,  ..., 1.7606e-03,\n",
       "          2.2887e-02, 1.7606e-03],\n",
       "         ...,\n",
       "         [5.7416e-02, 2.8708e-02, 4.7847e-03,  ..., 8.6124e-02,\n",
       "          3.3493e-02, 1.9139e-02],\n",
       "         [7.8960e-02, 1.8777e-01, 6.7405e-03,  ..., 4.8146e-04,\n",
       "          8.1849e-03, 1.9740e-02],\n",
       "         [8.4416e-02, 2.6840e-01, 2.1645e-03,  ..., 2.1645e-03,\n",
       "          2.8139e-02, 4.9784e-02]],\n",
       "\n",
       "        [[3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02],\n",
       "         [1.3506e-01, 1.7241e-02, 1.7241e-02,  ..., 1.4368e-02,\n",
       "          9.1954e-02, 1.4368e-02],\n",
       "         [3.0769e-02, 1.3846e-01, 1.5385e-02,  ..., 1.5385e-02,\n",
       "          1.5385e-01, 1.5385e-02],\n",
       "         ...,\n",
       "         [3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02],\n",
       "         [5.0909e-01, 4.5455e-02, 1.8182e-02,  ..., 9.0909e-03,\n",
       "          9.0909e-03, 9.0909e-03],\n",
       "         [3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02],\n",
       "         [8.4615e-02, 7.6923e-03, 2.3077e-02,  ..., 7.6923e-03,\n",
       "          8.4615e-02, 7.6923e-03],\n",
       "         [3.5714e-02, 3.5714e-02, 3.5714e-02,  ..., 3.5714e-02,\n",
       "          7.1429e-02, 3.5714e-02],\n",
       "         ...,\n",
       "         [2.9231e-01, 6.1538e-02, 1.5385e-02,  ..., 1.5385e-02,\n",
       "          3.0769e-02, 1.5385e-02],\n",
       "         [1.0526e-01, 8.7719e-02, 1.7544e-02,  ..., 1.7544e-02,\n",
       "          1.7544e-02, 1.7544e-02],\n",
       "         [2.1739e-02, 3.6957e-01, 2.1739e-02,  ..., 2.1739e-02,\n",
       "          2.1739e-02, 2.1739e-02]],\n",
       "\n",
       "        [[3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02],\n",
       "         [3.3041e-01, 2.1659e-02, 5.0691e-03,  ..., 1.8433e-03,\n",
       "          3.2258e-03, 1.0138e-02],\n",
       "         [5.5556e-02, 5.5556e-02, 1.8519e-02,  ..., 1.8519e-02,\n",
       "          1.8519e-02, 1.8519e-02],\n",
       "         ...,\n",
       "         [4.3636e-01, 3.6364e-02, 1.8182e-02,  ..., 3.6364e-02,\n",
       "          1.8182e-02, 1.8182e-02],\n",
       "         [4.0000e-02, 3.8000e-01, 2.0000e-02,  ..., 2.0000e-02,\n",
       "          2.0000e-02, 2.0000e-02],\n",
       "         [2.8571e-02, 2.6667e-01, 9.5238e-03,  ..., 1.9048e-02,\n",
       "          9.5238e-03, 9.5238e-03]],\n",
       "\n",
       "        [[3.7037e-02, 3.7037e-02, 3.7037e-02,  ..., 3.7037e-02,\n",
       "          3.7037e-02, 3.7037e-02],\n",
       "         [1.1161e-01, 1.6911e-02, 4.6223e-02,  ..., 4.5096e-03,\n",
       "          1.1048e-01, 4.5096e-03],\n",
       "         [3.2258e-02, 3.2258e-02, 3.2258e-02,  ..., 3.2258e-02,\n",
       "          3.2258e-02, 3.2258e-02],\n",
       "         ...,\n",
       "         [7.1429e-02, 3.5714e-02, 3.5714e-02,  ..., 3.5714e-02,\n",
       "          3.5714e-02, 3.5714e-02],\n",
       "         [2.0115e-01, 1.6092e-01, 5.7471e-03,  ..., 5.7471e-03,\n",
       "          5.7471e-03, 1.1494e-02],\n",
       "         [6.9444e-02, 1.9444e-01, 1.3889e-02,  ..., 1.3889e-02,\n",
       "          1.1111e-01, 1.3889e-02]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = (N + 1).float()\n",
    "P / P.sum(dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.tensor(\n",
    "    [\n",
    "        [25, 25, 50],  # .a\n",
    "        [30, 60, 10],  # .b\n",
    "        [10, 20, 70],  # .c\n",
    "        # ...\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.5000],\n",
       "        [0.3000, 0.6000, 0.1000],\n",
       "        [0.1000, 0.2000, 0.7000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpa = (tmp / tmp.sum(dim=1, keepdim=True))\n",
    "tmpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpa[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_2 = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [25, 25, 50],  # .aa\n",
    "            [30, 60, 10],  # .ab\n",
    "            [10, 20, 70],  # .ac\n",
    "            # ...\n",
    "        ],\n",
    "        [\n",
    "            [20, 25, 55],  # .ba\n",
    "            [15, 20, 65],  # .bb\n",
    "            [10, 10, 80],  # .bc\n",
    "            # ...\n",
    "        ],\n",
    "        [\n",
    "            [20, 30, 50],  # .ca\n",
    "            [33, 33, 33],  # .cb\n",
    "            [0, 66, 33],  # .cc\n",
    "            # ...\n",
    "        ],\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2500, 0.2500, 0.5000],\n",
       "         [0.3000, 0.6000, 0.1000],\n",
       "         [0.1000, 0.2000, 0.7000]],\n",
       "\n",
       "        [[0.2000, 0.2500, 0.5500],\n",
       "         [0.1500, 0.2000, 0.6500],\n",
       "         [0.1000, 0.1000, 0.8000]],\n",
       "\n",
       "        [[0.2000, 0.3000, 0.5000],\n",
       "         [0.3333, 0.3333, 0.3333],\n",
       "         [0.0000, 0.6667, 0.3333]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpb = (tmp_2 / tmp_2.sum(dim=2, keepdim=True))\n",
    "tmpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpb[0][0].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(239.5950)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P / P[0].sum(dim=1, keepdim=True)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide\n",
      "jakasid\n",
      "prelay\n",
      "adin\n",
      "kairritoper\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    i1x = 0\n",
    "    i2x = 0\n",
    "    char_list = []\n",
    "    while i2x != 0 or not char_list:\n",
    "        tmp = torch.multinomial(P[i1x][i2x], num_samples=1, replacement=True, generator=g).item()\n",
    "        i1x = i2x\n",
    "        i2x = tmp\n",
    "        char_list.append(i_to_s[i2x])\n",
    "\n",
    "    print(''.join(char_list[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Maximize likelihood of data w.r.t. model parameters (statistical model)\n",
    "# Equivalent to maximizing log-likelihood (because log is monotonic)\n",
    "# Equivalent to minimizing negative log-likelihood\n",
    "# Equivalent to minimizing the average negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-22.4022)\n",
      "nll=tensor(22.4022)\n",
      "2.8002805709838867\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "# for w in words:\n",
    "for w in [\"andrejq\"]:\n",
    "    chars = [\".\"]*2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chars, chars[1:], chars[2:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        ix3 = s_to_i[ch3]\n",
    "        prob = P[ix1, ix2, ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "print(f\"{log_likelihood=}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"{nll=}\")\n",
    "print(f\"{nll/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Raw 2nd approach - 729 x 27 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27*27,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..',\n",
       " '.a',\n",
       " '.b',\n",
       " '.c',\n",
       " '.d',\n",
       " '.e',\n",
       " '.f',\n",
       " '.g',\n",
       " '.h',\n",
       " '.i',\n",
       " '.j',\n",
       " '.k',\n",
       " '.l',\n",
       " '.m',\n",
       " '.n',\n",
       " '.o',\n",
       " '.p',\n",
       " '.q',\n",
       " '.r',\n",
       " '.s',\n",
       " '.t',\n",
       " '.u',\n",
       " '.v',\n",
       " '.w',\n",
       " '.x',\n",
       " '.y',\n",
       " '.z',\n",
       " 'a.',\n",
       " 'aa',\n",
       " 'ab',\n",
       " 'ac',\n",
       " 'ad',\n",
       " 'ae',\n",
       " 'af',\n",
       " 'ag',\n",
       " 'ah',\n",
       " 'ai',\n",
       " 'aj',\n",
       " 'ak',\n",
       " 'al',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ao',\n",
       " 'ap',\n",
       " 'aq',\n",
       " 'ar',\n",
       " 'as',\n",
       " 'at',\n",
       " 'au',\n",
       " 'av',\n",
       " 'aw',\n",
       " 'ax',\n",
       " 'ay',\n",
       " 'az',\n",
       " 'b.',\n",
       " 'ba',\n",
       " 'bb',\n",
       " 'bc',\n",
       " 'bd',\n",
       " 'be',\n",
       " 'bf',\n",
       " 'bg',\n",
       " 'bh',\n",
       " 'bi',\n",
       " 'bj',\n",
       " 'bk',\n",
       " 'bl',\n",
       " 'bm',\n",
       " 'bn',\n",
       " 'bo',\n",
       " 'bp',\n",
       " 'bq',\n",
       " 'br',\n",
       " 'bs',\n",
       " 'bt',\n",
       " 'bu',\n",
       " 'bv',\n",
       " 'bw',\n",
       " 'bx',\n",
       " 'by',\n",
       " 'bz',\n",
       " 'c.',\n",
       " 'ca',\n",
       " 'cb',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'ce',\n",
       " 'cf',\n",
       " 'cg',\n",
       " 'ch',\n",
       " 'ci',\n",
       " 'cj',\n",
       " 'ck',\n",
       " 'cl',\n",
       " 'cm',\n",
       " 'cn',\n",
       " 'co',\n",
       " 'cp',\n",
       " 'cq',\n",
       " 'cr',\n",
       " 'cs',\n",
       " 'ct',\n",
       " 'cu',\n",
       " 'cv',\n",
       " 'cw',\n",
       " 'cx',\n",
       " 'cy',\n",
       " 'cz',\n",
       " 'd.',\n",
       " 'da',\n",
       " 'db',\n",
       " 'dc',\n",
       " 'dd',\n",
       " 'de',\n",
       " 'df',\n",
       " 'dg',\n",
       " 'dh',\n",
       " 'di',\n",
       " 'dj',\n",
       " 'dk',\n",
       " 'dl',\n",
       " 'dm',\n",
       " 'dn',\n",
       " 'do',\n",
       " 'dp',\n",
       " 'dq',\n",
       " 'dr',\n",
       " 'ds',\n",
       " 'dt',\n",
       " 'du',\n",
       " 'dv',\n",
       " 'dw',\n",
       " 'dx',\n",
       " 'dy',\n",
       " 'dz',\n",
       " 'e.',\n",
       " 'ea',\n",
       " 'eb',\n",
       " 'ec',\n",
       " 'ed',\n",
       " 'ee',\n",
       " 'ef',\n",
       " 'eg',\n",
       " 'eh',\n",
       " 'ei',\n",
       " 'ej',\n",
       " 'ek',\n",
       " 'el',\n",
       " 'em',\n",
       " 'en',\n",
       " 'eo',\n",
       " 'ep',\n",
       " 'eq',\n",
       " 'er',\n",
       " 'es',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'ev',\n",
       " 'ew',\n",
       " 'ex',\n",
       " 'ey',\n",
       " 'ez',\n",
       " 'f.',\n",
       " 'fa',\n",
       " 'fb',\n",
       " 'fc',\n",
       " 'fd',\n",
       " 'fe',\n",
       " 'ff',\n",
       " 'fg',\n",
       " 'fh',\n",
       " 'fi',\n",
       " 'fj',\n",
       " 'fk',\n",
       " 'fl',\n",
       " 'fm',\n",
       " 'fn',\n",
       " 'fo',\n",
       " 'fp',\n",
       " 'fq',\n",
       " 'fr',\n",
       " 'fs',\n",
       " 'ft',\n",
       " 'fu',\n",
       " 'fv',\n",
       " 'fw',\n",
       " 'fx',\n",
       " 'fy',\n",
       " 'fz',\n",
       " 'g.',\n",
       " 'ga',\n",
       " 'gb',\n",
       " 'gc',\n",
       " 'gd',\n",
       " 'ge',\n",
       " 'gf',\n",
       " 'gg',\n",
       " 'gh',\n",
       " 'gi',\n",
       " 'gj',\n",
       " 'gk',\n",
       " 'gl',\n",
       " 'gm',\n",
       " 'gn',\n",
       " 'go',\n",
       " 'gp',\n",
       " 'gq',\n",
       " 'gr',\n",
       " 'gs',\n",
       " 'gt',\n",
       " 'gu',\n",
       " 'gv',\n",
       " 'gw',\n",
       " 'gx',\n",
       " 'gy',\n",
       " 'gz',\n",
       " 'h.',\n",
       " 'ha',\n",
       " 'hb',\n",
       " 'hc',\n",
       " 'hd',\n",
       " 'he',\n",
       " 'hf',\n",
       " 'hg',\n",
       " 'hh',\n",
       " 'hi',\n",
       " 'hj',\n",
       " 'hk',\n",
       " 'hl',\n",
       " 'hm',\n",
       " 'hn',\n",
       " 'ho',\n",
       " 'hp',\n",
       " 'hq',\n",
       " 'hr',\n",
       " 'hs',\n",
       " 'ht',\n",
       " 'hu',\n",
       " 'hv',\n",
       " 'hw',\n",
       " 'hx',\n",
       " 'hy',\n",
       " 'hz',\n",
       " 'i.',\n",
       " 'ia',\n",
       " 'ib',\n",
       " 'ic',\n",
       " 'id',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ig',\n",
       " 'ih',\n",
       " 'ii',\n",
       " 'ij',\n",
       " 'ik',\n",
       " 'il',\n",
       " 'im',\n",
       " 'in',\n",
       " 'io',\n",
       " 'ip',\n",
       " 'iq',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'it',\n",
       " 'iu',\n",
       " 'iv',\n",
       " 'iw',\n",
       " 'ix',\n",
       " 'iy',\n",
       " 'iz',\n",
       " 'j.',\n",
       " 'ja',\n",
       " 'jb',\n",
       " 'jc',\n",
       " 'jd',\n",
       " 'je',\n",
       " 'jf',\n",
       " 'jg',\n",
       " 'jh',\n",
       " 'ji',\n",
       " 'jj',\n",
       " 'jk',\n",
       " 'jl',\n",
       " 'jm',\n",
       " 'jn',\n",
       " 'jo',\n",
       " 'jp',\n",
       " 'jq',\n",
       " 'jr',\n",
       " 'js',\n",
       " 'jt',\n",
       " 'ju',\n",
       " 'jv',\n",
       " 'jw',\n",
       " 'jx',\n",
       " 'jy',\n",
       " 'jz',\n",
       " 'k.',\n",
       " 'ka',\n",
       " 'kb',\n",
       " 'kc',\n",
       " 'kd',\n",
       " 'ke',\n",
       " 'kf',\n",
       " 'kg',\n",
       " 'kh',\n",
       " 'ki',\n",
       " 'kj',\n",
       " 'kk',\n",
       " 'kl',\n",
       " 'km',\n",
       " 'kn',\n",
       " 'ko',\n",
       " 'kp',\n",
       " 'kq',\n",
       " 'kr',\n",
       " 'ks',\n",
       " 'kt',\n",
       " 'ku',\n",
       " 'kv',\n",
       " 'kw',\n",
       " 'kx',\n",
       " 'ky',\n",
       " 'kz',\n",
       " 'l.',\n",
       " 'la',\n",
       " 'lb',\n",
       " 'lc',\n",
       " 'ld',\n",
       " 'le',\n",
       " 'lf',\n",
       " 'lg',\n",
       " 'lh',\n",
       " 'li',\n",
       " 'lj',\n",
       " 'lk',\n",
       " 'll',\n",
       " 'lm',\n",
       " 'ln',\n",
       " 'lo',\n",
       " 'lp',\n",
       " 'lq',\n",
       " 'lr',\n",
       " 'ls',\n",
       " 'lt',\n",
       " 'lu',\n",
       " 'lv',\n",
       " 'lw',\n",
       " 'lx',\n",
       " 'ly',\n",
       " 'lz',\n",
       " 'm.',\n",
       " 'ma',\n",
       " 'mb',\n",
       " 'mc',\n",
       " 'md',\n",
       " 'me',\n",
       " 'mf',\n",
       " 'mg',\n",
       " 'mh',\n",
       " 'mi',\n",
       " 'mj',\n",
       " 'mk',\n",
       " 'ml',\n",
       " 'mm',\n",
       " 'mn',\n",
       " 'mo',\n",
       " 'mp',\n",
       " 'mq',\n",
       " 'mr',\n",
       " 'ms',\n",
       " 'mt',\n",
       " 'mu',\n",
       " 'mv',\n",
       " 'mw',\n",
       " 'mx',\n",
       " 'my',\n",
       " 'mz',\n",
       " 'n.',\n",
       " 'na',\n",
       " 'nb',\n",
       " 'nc',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'nf',\n",
       " 'ng',\n",
       " 'nh',\n",
       " 'ni',\n",
       " 'nj',\n",
       " 'nk',\n",
       " 'nl',\n",
       " 'nm',\n",
       " 'nn',\n",
       " 'no',\n",
       " 'np',\n",
       " 'nq',\n",
       " 'nr',\n",
       " 'ns',\n",
       " 'nt',\n",
       " 'nu',\n",
       " 'nv',\n",
       " 'nw',\n",
       " 'nx',\n",
       " 'ny',\n",
       " 'nz',\n",
       " 'o.',\n",
       " 'oa',\n",
       " 'ob',\n",
       " 'oc',\n",
       " 'od',\n",
       " 'oe',\n",
       " 'of',\n",
       " 'og',\n",
       " 'oh',\n",
       " 'oi',\n",
       " 'oj',\n",
       " 'ok',\n",
       " 'ol',\n",
       " 'om',\n",
       " 'on',\n",
       " 'oo',\n",
       " 'op',\n",
       " 'oq',\n",
       " 'or',\n",
       " 'os',\n",
       " 'ot',\n",
       " 'ou',\n",
       " 'ov',\n",
       " 'ow',\n",
       " 'ox',\n",
       " 'oy',\n",
       " 'oz',\n",
       " 'p.',\n",
       " 'pa',\n",
       " 'pb',\n",
       " 'pc',\n",
       " 'pd',\n",
       " 'pe',\n",
       " 'pf',\n",
       " 'pg',\n",
       " 'ph',\n",
       " 'pi',\n",
       " 'pj',\n",
       " 'pk',\n",
       " 'pl',\n",
       " 'pm',\n",
       " 'pn',\n",
       " 'po',\n",
       " 'pp',\n",
       " 'pq',\n",
       " 'pr',\n",
       " 'ps',\n",
       " 'pt',\n",
       " 'pu',\n",
       " 'pv',\n",
       " 'pw',\n",
       " 'px',\n",
       " 'py',\n",
       " 'pz',\n",
       " 'q.',\n",
       " 'qa',\n",
       " 'qb',\n",
       " 'qc',\n",
       " 'qd',\n",
       " 'qe',\n",
       " 'qf',\n",
       " 'qg',\n",
       " 'qh',\n",
       " 'qi',\n",
       " 'qj',\n",
       " 'qk',\n",
       " 'ql',\n",
       " 'qm',\n",
       " 'qn',\n",
       " 'qo',\n",
       " 'qp',\n",
       " 'qq',\n",
       " 'qr',\n",
       " 'qs',\n",
       " 'qt',\n",
       " 'qu',\n",
       " 'qv',\n",
       " 'qw',\n",
       " 'qx',\n",
       " 'qy',\n",
       " 'qz',\n",
       " 'r.',\n",
       " 'ra',\n",
       " 'rb',\n",
       " 'rc',\n",
       " 'rd',\n",
       " 're',\n",
       " 'rf',\n",
       " 'rg',\n",
       " 'rh',\n",
       " 'ri',\n",
       " 'rj',\n",
       " 'rk',\n",
       " 'rl',\n",
       " 'rm',\n",
       " 'rn',\n",
       " 'ro',\n",
       " 'rp',\n",
       " 'rq',\n",
       " 'rr',\n",
       " 'rs',\n",
       " 'rt',\n",
       " 'ru',\n",
       " 'rv',\n",
       " 'rw',\n",
       " 'rx',\n",
       " 'ry',\n",
       " 'rz',\n",
       " 's.',\n",
       " 'sa',\n",
       " 'sb',\n",
       " 'sc',\n",
       " 'sd',\n",
       " 'se',\n",
       " 'sf',\n",
       " 'sg',\n",
       " 'sh',\n",
       " 'si',\n",
       " 'sj',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'sm',\n",
       " 'sn',\n",
       " 'so',\n",
       " 'sp',\n",
       " 'sq',\n",
       " 'sr',\n",
       " 'ss',\n",
       " 'st',\n",
       " 'su',\n",
       " 'sv',\n",
       " 'sw',\n",
       " 'sx',\n",
       " 'sy',\n",
       " 'sz',\n",
       " 't.',\n",
       " 'ta',\n",
       " 'tb',\n",
       " 'tc',\n",
       " 'td',\n",
       " 'te',\n",
       " 'tf',\n",
       " 'tg',\n",
       " 'th',\n",
       " 'ti',\n",
       " 'tj',\n",
       " 'tk',\n",
       " 'tl',\n",
       " 'tm',\n",
       " 'tn',\n",
       " 'to',\n",
       " 'tp',\n",
       " 'tq',\n",
       " 'tr',\n",
       " 'ts',\n",
       " 'tt',\n",
       " 'tu',\n",
       " 'tv',\n",
       " 'tw',\n",
       " 'tx',\n",
       " 'ty',\n",
       " 'tz',\n",
       " 'u.',\n",
       " 'ua',\n",
       " 'ub',\n",
       " 'uc',\n",
       " 'ud',\n",
       " 'ue',\n",
       " 'uf',\n",
       " 'ug',\n",
       " 'uh',\n",
       " 'ui',\n",
       " 'uj',\n",
       " 'uk',\n",
       " 'ul',\n",
       " 'um',\n",
       " 'un',\n",
       " 'uo',\n",
       " 'up',\n",
       " 'uq',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'ut',\n",
       " 'uu',\n",
       " 'uv',\n",
       " 'uw',\n",
       " 'ux',\n",
       " 'uy',\n",
       " 'uz',\n",
       " 'v.',\n",
       " 'va',\n",
       " 'vb',\n",
       " 'vc',\n",
       " 'vd',\n",
       " 've',\n",
       " 'vf',\n",
       " 'vg',\n",
       " 'vh',\n",
       " 'vi',\n",
       " 'vj',\n",
       " 'vk',\n",
       " 'vl',\n",
       " 'vm',\n",
       " 'vn',\n",
       " 'vo',\n",
       " 'vp',\n",
       " 'vq',\n",
       " 'vr',\n",
       " 'vs',\n",
       " 'vt',\n",
       " 'vu',\n",
       " 'vv',\n",
       " 'vw',\n",
       " 'vx',\n",
       " 'vy',\n",
       " 'vz',\n",
       " 'w.',\n",
       " 'wa',\n",
       " 'wb',\n",
       " 'wc',\n",
       " 'wd',\n",
       " 'we',\n",
       " 'wf',\n",
       " 'wg',\n",
       " 'wh',\n",
       " 'wi',\n",
       " 'wj',\n",
       " 'wk',\n",
       " 'wl',\n",
       " 'wm',\n",
       " 'wn',\n",
       " 'wo',\n",
       " 'wp',\n",
       " 'wq',\n",
       " 'wr',\n",
       " 'ws',\n",
       " 'wt',\n",
       " 'wu',\n",
       " 'wv',\n",
       " 'ww',\n",
       " 'wx',\n",
       " 'wy',\n",
       " 'wz',\n",
       " 'x.',\n",
       " 'xa',\n",
       " 'xb',\n",
       " 'xc',\n",
       " 'xd',\n",
       " 'xe',\n",
       " 'xf',\n",
       " 'xg',\n",
       " 'xh',\n",
       " 'xi',\n",
       " 'xj',\n",
       " 'xk',\n",
       " 'xl',\n",
       " 'xm',\n",
       " 'xn',\n",
       " 'xo',\n",
       " 'xp',\n",
       " 'xq',\n",
       " 'xr',\n",
       " 'xs',\n",
       " 'xt',\n",
       " 'xu',\n",
       " 'xv',\n",
       " 'xw',\n",
       " 'xx',\n",
       " 'xy',\n",
       " 'xz',\n",
       " 'y.',\n",
       " 'ya',\n",
       " 'yb',\n",
       " 'yc',\n",
       " 'yd',\n",
       " 'ye',\n",
       " 'yf',\n",
       " 'yg',\n",
       " 'yh',\n",
       " 'yi',\n",
       " 'yj',\n",
       " 'yk',\n",
       " 'yl',\n",
       " 'ym',\n",
       " 'yn',\n",
       " 'yo',\n",
       " 'yp',\n",
       " 'yq',\n",
       " 'yr',\n",
       " 'ys',\n",
       " 'yt',\n",
       " 'yu',\n",
       " 'yv',\n",
       " 'yw',\n",
       " 'yx',\n",
       " 'yy',\n",
       " 'yz',\n",
       " 'z.',\n",
       " 'za',\n",
       " 'zb',\n",
       " 'zc',\n",
       " 'zd',\n",
       " 'ze',\n",
       " 'zf',\n",
       " 'zg',\n",
       " 'zh',\n",
       " 'zi',\n",
       " 'zj',\n",
       " 'zk',\n",
       " 'zl',\n",
       " 'zm',\n",
       " 'zn',\n",
       " 'zo',\n",
       " 'zp',\n",
       " 'zq',\n",
       " 'zr',\n",
       " 'zs',\n",
       " 'zt',\n",
       " 'zu',\n",
       " 'zv',\n",
       " 'zw',\n",
       " 'zx',\n",
       " 'zy',\n",
       " 'zz']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = [\".\"] + sorted(list(set(''.join(words))))\n",
    "char_tuples = [ch1 + ch2 for ch1 in chars for ch2 in chars]\n",
    "char_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '..',\n",
       " 1: '.a',\n",
       " 2: '.b',\n",
       " 3: '.c',\n",
       " 4: '.d',\n",
       " 5: '.e',\n",
       " 6: '.f',\n",
       " 7: '.g',\n",
       " 8: '.h',\n",
       " 9: '.i',\n",
       " 10: '.j',\n",
       " 11: '.k',\n",
       " 12: '.l',\n",
       " 13: '.m',\n",
       " 14: '.n',\n",
       " 15: '.o',\n",
       " 16: '.p',\n",
       " 17: '.q',\n",
       " 18: '.r',\n",
       " 19: '.s',\n",
       " 20: '.t',\n",
       " 21: '.u',\n",
       " 22: '.v',\n",
       " 23: '.w',\n",
       " 24: '.x',\n",
       " 25: '.y',\n",
       " 26: '.z',\n",
       " 27: 'a.',\n",
       " 28: 'aa',\n",
       " 29: 'ab',\n",
       " 30: 'ac',\n",
       " 31: 'ad',\n",
       " 32: 'ae',\n",
       " 33: 'af',\n",
       " 34: 'ag',\n",
       " 35: 'ah',\n",
       " 36: 'ai',\n",
       " 37: 'aj',\n",
       " 38: 'ak',\n",
       " 39: 'al',\n",
       " 40: 'am',\n",
       " 41: 'an',\n",
       " 42: 'ao',\n",
       " 43: 'ap',\n",
       " 44: 'aq',\n",
       " 45: 'ar',\n",
       " 46: 'as',\n",
       " 47: 'at',\n",
       " 48: 'au',\n",
       " 49: 'av',\n",
       " 50: 'aw',\n",
       " 51: 'ax',\n",
       " 52: 'ay',\n",
       " 53: 'az',\n",
       " 54: 'b.',\n",
       " 55: 'ba',\n",
       " 56: 'bb',\n",
       " 57: 'bc',\n",
       " 58: 'bd',\n",
       " 59: 'be',\n",
       " 60: 'bf',\n",
       " 61: 'bg',\n",
       " 62: 'bh',\n",
       " 63: 'bi',\n",
       " 64: 'bj',\n",
       " 65: 'bk',\n",
       " 66: 'bl',\n",
       " 67: 'bm',\n",
       " 68: 'bn',\n",
       " 69: 'bo',\n",
       " 70: 'bp',\n",
       " 71: 'bq',\n",
       " 72: 'br',\n",
       " 73: 'bs',\n",
       " 74: 'bt',\n",
       " 75: 'bu',\n",
       " 76: 'bv',\n",
       " 77: 'bw',\n",
       " 78: 'bx',\n",
       " 79: 'by',\n",
       " 80: 'bz',\n",
       " 81: 'c.',\n",
       " 82: 'ca',\n",
       " 83: 'cb',\n",
       " 84: 'cc',\n",
       " 85: 'cd',\n",
       " 86: 'ce',\n",
       " 87: 'cf',\n",
       " 88: 'cg',\n",
       " 89: 'ch',\n",
       " 90: 'ci',\n",
       " 91: 'cj',\n",
       " 92: 'ck',\n",
       " 93: 'cl',\n",
       " 94: 'cm',\n",
       " 95: 'cn',\n",
       " 96: 'co',\n",
       " 97: 'cp',\n",
       " 98: 'cq',\n",
       " 99: 'cr',\n",
       " 100: 'cs',\n",
       " 101: 'ct',\n",
       " 102: 'cu',\n",
       " 103: 'cv',\n",
       " 104: 'cw',\n",
       " 105: 'cx',\n",
       " 106: 'cy',\n",
       " 107: 'cz',\n",
       " 108: 'd.',\n",
       " 109: 'da',\n",
       " 110: 'db',\n",
       " 111: 'dc',\n",
       " 112: 'dd',\n",
       " 113: 'de',\n",
       " 114: 'df',\n",
       " 115: 'dg',\n",
       " 116: 'dh',\n",
       " 117: 'di',\n",
       " 118: 'dj',\n",
       " 119: 'dk',\n",
       " 120: 'dl',\n",
       " 121: 'dm',\n",
       " 122: 'dn',\n",
       " 123: 'do',\n",
       " 124: 'dp',\n",
       " 125: 'dq',\n",
       " 126: 'dr',\n",
       " 127: 'ds',\n",
       " 128: 'dt',\n",
       " 129: 'du',\n",
       " 130: 'dv',\n",
       " 131: 'dw',\n",
       " 132: 'dx',\n",
       " 133: 'dy',\n",
       " 134: 'dz',\n",
       " 135: 'e.',\n",
       " 136: 'ea',\n",
       " 137: 'eb',\n",
       " 138: 'ec',\n",
       " 139: 'ed',\n",
       " 140: 'ee',\n",
       " 141: 'ef',\n",
       " 142: 'eg',\n",
       " 143: 'eh',\n",
       " 144: 'ei',\n",
       " 145: 'ej',\n",
       " 146: 'ek',\n",
       " 147: 'el',\n",
       " 148: 'em',\n",
       " 149: 'en',\n",
       " 150: 'eo',\n",
       " 151: 'ep',\n",
       " 152: 'eq',\n",
       " 153: 'er',\n",
       " 154: 'es',\n",
       " 155: 'et',\n",
       " 156: 'eu',\n",
       " 157: 'ev',\n",
       " 158: 'ew',\n",
       " 159: 'ex',\n",
       " 160: 'ey',\n",
       " 161: 'ez',\n",
       " 162: 'f.',\n",
       " 163: 'fa',\n",
       " 164: 'fb',\n",
       " 165: 'fc',\n",
       " 166: 'fd',\n",
       " 167: 'fe',\n",
       " 168: 'ff',\n",
       " 169: 'fg',\n",
       " 170: 'fh',\n",
       " 171: 'fi',\n",
       " 172: 'fj',\n",
       " 173: 'fk',\n",
       " 174: 'fl',\n",
       " 175: 'fm',\n",
       " 176: 'fn',\n",
       " 177: 'fo',\n",
       " 178: 'fp',\n",
       " 179: 'fq',\n",
       " 180: 'fr',\n",
       " 181: 'fs',\n",
       " 182: 'ft',\n",
       " 183: 'fu',\n",
       " 184: 'fv',\n",
       " 185: 'fw',\n",
       " 186: 'fx',\n",
       " 187: 'fy',\n",
       " 188: 'fz',\n",
       " 189: 'g.',\n",
       " 190: 'ga',\n",
       " 191: 'gb',\n",
       " 192: 'gc',\n",
       " 193: 'gd',\n",
       " 194: 'ge',\n",
       " 195: 'gf',\n",
       " 196: 'gg',\n",
       " 197: 'gh',\n",
       " 198: 'gi',\n",
       " 199: 'gj',\n",
       " 200: 'gk',\n",
       " 201: 'gl',\n",
       " 202: 'gm',\n",
       " 203: 'gn',\n",
       " 204: 'go',\n",
       " 205: 'gp',\n",
       " 206: 'gq',\n",
       " 207: 'gr',\n",
       " 208: 'gs',\n",
       " 209: 'gt',\n",
       " 210: 'gu',\n",
       " 211: 'gv',\n",
       " 212: 'gw',\n",
       " 213: 'gx',\n",
       " 214: 'gy',\n",
       " 215: 'gz',\n",
       " 216: 'h.',\n",
       " 217: 'ha',\n",
       " 218: 'hb',\n",
       " 219: 'hc',\n",
       " 220: 'hd',\n",
       " 221: 'he',\n",
       " 222: 'hf',\n",
       " 223: 'hg',\n",
       " 224: 'hh',\n",
       " 225: 'hi',\n",
       " 226: 'hj',\n",
       " 227: 'hk',\n",
       " 228: 'hl',\n",
       " 229: 'hm',\n",
       " 230: 'hn',\n",
       " 231: 'ho',\n",
       " 232: 'hp',\n",
       " 233: 'hq',\n",
       " 234: 'hr',\n",
       " 235: 'hs',\n",
       " 236: 'ht',\n",
       " 237: 'hu',\n",
       " 238: 'hv',\n",
       " 239: 'hw',\n",
       " 240: 'hx',\n",
       " 241: 'hy',\n",
       " 242: 'hz',\n",
       " 243: 'i.',\n",
       " 244: 'ia',\n",
       " 245: 'ib',\n",
       " 246: 'ic',\n",
       " 247: 'id',\n",
       " 248: 'ie',\n",
       " 249: 'if',\n",
       " 250: 'ig',\n",
       " 251: 'ih',\n",
       " 252: 'ii',\n",
       " 253: 'ij',\n",
       " 254: 'ik',\n",
       " 255: 'il',\n",
       " 256: 'im',\n",
       " 257: 'in',\n",
       " 258: 'io',\n",
       " 259: 'ip',\n",
       " 260: 'iq',\n",
       " 261: 'ir',\n",
       " 262: 'is',\n",
       " 263: 'it',\n",
       " 264: 'iu',\n",
       " 265: 'iv',\n",
       " 266: 'iw',\n",
       " 267: 'ix',\n",
       " 268: 'iy',\n",
       " 269: 'iz',\n",
       " 270: 'j.',\n",
       " 271: 'ja',\n",
       " 272: 'jb',\n",
       " 273: 'jc',\n",
       " 274: 'jd',\n",
       " 275: 'je',\n",
       " 276: 'jf',\n",
       " 277: 'jg',\n",
       " 278: 'jh',\n",
       " 279: 'ji',\n",
       " 280: 'jj',\n",
       " 281: 'jk',\n",
       " 282: 'jl',\n",
       " 283: 'jm',\n",
       " 284: 'jn',\n",
       " 285: 'jo',\n",
       " 286: 'jp',\n",
       " 287: 'jq',\n",
       " 288: 'jr',\n",
       " 289: 'js',\n",
       " 290: 'jt',\n",
       " 291: 'ju',\n",
       " 292: 'jv',\n",
       " 293: 'jw',\n",
       " 294: 'jx',\n",
       " 295: 'jy',\n",
       " 296: 'jz',\n",
       " 297: 'k.',\n",
       " 298: 'ka',\n",
       " 299: 'kb',\n",
       " 300: 'kc',\n",
       " 301: 'kd',\n",
       " 302: 'ke',\n",
       " 303: 'kf',\n",
       " 304: 'kg',\n",
       " 305: 'kh',\n",
       " 306: 'ki',\n",
       " 307: 'kj',\n",
       " 308: 'kk',\n",
       " 309: 'kl',\n",
       " 310: 'km',\n",
       " 311: 'kn',\n",
       " 312: 'ko',\n",
       " 313: 'kp',\n",
       " 314: 'kq',\n",
       " 315: 'kr',\n",
       " 316: 'ks',\n",
       " 317: 'kt',\n",
       " 318: 'ku',\n",
       " 319: 'kv',\n",
       " 320: 'kw',\n",
       " 321: 'kx',\n",
       " 322: 'ky',\n",
       " 323: 'kz',\n",
       " 324: 'l.',\n",
       " 325: 'la',\n",
       " 326: 'lb',\n",
       " 327: 'lc',\n",
       " 328: 'ld',\n",
       " 329: 'le',\n",
       " 330: 'lf',\n",
       " 331: 'lg',\n",
       " 332: 'lh',\n",
       " 333: 'li',\n",
       " 334: 'lj',\n",
       " 335: 'lk',\n",
       " 336: 'll',\n",
       " 337: 'lm',\n",
       " 338: 'ln',\n",
       " 339: 'lo',\n",
       " 340: 'lp',\n",
       " 341: 'lq',\n",
       " 342: 'lr',\n",
       " 343: 'ls',\n",
       " 344: 'lt',\n",
       " 345: 'lu',\n",
       " 346: 'lv',\n",
       " 347: 'lw',\n",
       " 348: 'lx',\n",
       " 349: 'ly',\n",
       " 350: 'lz',\n",
       " 351: 'm.',\n",
       " 352: 'ma',\n",
       " 353: 'mb',\n",
       " 354: 'mc',\n",
       " 355: 'md',\n",
       " 356: 'me',\n",
       " 357: 'mf',\n",
       " 358: 'mg',\n",
       " 359: 'mh',\n",
       " 360: 'mi',\n",
       " 361: 'mj',\n",
       " 362: 'mk',\n",
       " 363: 'ml',\n",
       " 364: 'mm',\n",
       " 365: 'mn',\n",
       " 366: 'mo',\n",
       " 367: 'mp',\n",
       " 368: 'mq',\n",
       " 369: 'mr',\n",
       " 370: 'ms',\n",
       " 371: 'mt',\n",
       " 372: 'mu',\n",
       " 373: 'mv',\n",
       " 374: 'mw',\n",
       " 375: 'mx',\n",
       " 376: 'my',\n",
       " 377: 'mz',\n",
       " 378: 'n.',\n",
       " 379: 'na',\n",
       " 380: 'nb',\n",
       " 381: 'nc',\n",
       " 382: 'nd',\n",
       " 383: 'ne',\n",
       " 384: 'nf',\n",
       " 385: 'ng',\n",
       " 386: 'nh',\n",
       " 387: 'ni',\n",
       " 388: 'nj',\n",
       " 389: 'nk',\n",
       " 390: 'nl',\n",
       " 391: 'nm',\n",
       " 392: 'nn',\n",
       " 393: 'no',\n",
       " 394: 'np',\n",
       " 395: 'nq',\n",
       " 396: 'nr',\n",
       " 397: 'ns',\n",
       " 398: 'nt',\n",
       " 399: 'nu',\n",
       " 400: 'nv',\n",
       " 401: 'nw',\n",
       " 402: 'nx',\n",
       " 403: 'ny',\n",
       " 404: 'nz',\n",
       " 405: 'o.',\n",
       " 406: 'oa',\n",
       " 407: 'ob',\n",
       " 408: 'oc',\n",
       " 409: 'od',\n",
       " 410: 'oe',\n",
       " 411: 'of',\n",
       " 412: 'og',\n",
       " 413: 'oh',\n",
       " 414: 'oi',\n",
       " 415: 'oj',\n",
       " 416: 'ok',\n",
       " 417: 'ol',\n",
       " 418: 'om',\n",
       " 419: 'on',\n",
       " 420: 'oo',\n",
       " 421: 'op',\n",
       " 422: 'oq',\n",
       " 423: 'or',\n",
       " 424: 'os',\n",
       " 425: 'ot',\n",
       " 426: 'ou',\n",
       " 427: 'ov',\n",
       " 428: 'ow',\n",
       " 429: 'ox',\n",
       " 430: 'oy',\n",
       " 431: 'oz',\n",
       " 432: 'p.',\n",
       " 433: 'pa',\n",
       " 434: 'pb',\n",
       " 435: 'pc',\n",
       " 436: 'pd',\n",
       " 437: 'pe',\n",
       " 438: 'pf',\n",
       " 439: 'pg',\n",
       " 440: 'ph',\n",
       " 441: 'pi',\n",
       " 442: 'pj',\n",
       " 443: 'pk',\n",
       " 444: 'pl',\n",
       " 445: 'pm',\n",
       " 446: 'pn',\n",
       " 447: 'po',\n",
       " 448: 'pp',\n",
       " 449: 'pq',\n",
       " 450: 'pr',\n",
       " 451: 'ps',\n",
       " 452: 'pt',\n",
       " 453: 'pu',\n",
       " 454: 'pv',\n",
       " 455: 'pw',\n",
       " 456: 'px',\n",
       " 457: 'py',\n",
       " 458: 'pz',\n",
       " 459: 'q.',\n",
       " 460: 'qa',\n",
       " 461: 'qb',\n",
       " 462: 'qc',\n",
       " 463: 'qd',\n",
       " 464: 'qe',\n",
       " 465: 'qf',\n",
       " 466: 'qg',\n",
       " 467: 'qh',\n",
       " 468: 'qi',\n",
       " 469: 'qj',\n",
       " 470: 'qk',\n",
       " 471: 'ql',\n",
       " 472: 'qm',\n",
       " 473: 'qn',\n",
       " 474: 'qo',\n",
       " 475: 'qp',\n",
       " 476: 'qq',\n",
       " 477: 'qr',\n",
       " 478: 'qs',\n",
       " 479: 'qt',\n",
       " 480: 'qu',\n",
       " 481: 'qv',\n",
       " 482: 'qw',\n",
       " 483: 'qx',\n",
       " 484: 'qy',\n",
       " 485: 'qz',\n",
       " 486: 'r.',\n",
       " 487: 'ra',\n",
       " 488: 'rb',\n",
       " 489: 'rc',\n",
       " 490: 'rd',\n",
       " 491: 're',\n",
       " 492: 'rf',\n",
       " 493: 'rg',\n",
       " 494: 'rh',\n",
       " 495: 'ri',\n",
       " 496: 'rj',\n",
       " 497: 'rk',\n",
       " 498: 'rl',\n",
       " 499: 'rm',\n",
       " 500: 'rn',\n",
       " 501: 'ro',\n",
       " 502: 'rp',\n",
       " 503: 'rq',\n",
       " 504: 'rr',\n",
       " 505: 'rs',\n",
       " 506: 'rt',\n",
       " 507: 'ru',\n",
       " 508: 'rv',\n",
       " 509: 'rw',\n",
       " 510: 'rx',\n",
       " 511: 'ry',\n",
       " 512: 'rz',\n",
       " 513: 's.',\n",
       " 514: 'sa',\n",
       " 515: 'sb',\n",
       " 516: 'sc',\n",
       " 517: 'sd',\n",
       " 518: 'se',\n",
       " 519: 'sf',\n",
       " 520: 'sg',\n",
       " 521: 'sh',\n",
       " 522: 'si',\n",
       " 523: 'sj',\n",
       " 524: 'sk',\n",
       " 525: 'sl',\n",
       " 526: 'sm',\n",
       " 527: 'sn',\n",
       " 528: 'so',\n",
       " 529: 'sp',\n",
       " 530: 'sq',\n",
       " 531: 'sr',\n",
       " 532: 'ss',\n",
       " 533: 'st',\n",
       " 534: 'su',\n",
       " 535: 'sv',\n",
       " 536: 'sw',\n",
       " 537: 'sx',\n",
       " 538: 'sy',\n",
       " 539: 'sz',\n",
       " 540: 't.',\n",
       " 541: 'ta',\n",
       " 542: 'tb',\n",
       " 543: 'tc',\n",
       " 544: 'td',\n",
       " 545: 'te',\n",
       " 546: 'tf',\n",
       " 547: 'tg',\n",
       " 548: 'th',\n",
       " 549: 'ti',\n",
       " 550: 'tj',\n",
       " 551: 'tk',\n",
       " 552: 'tl',\n",
       " 553: 'tm',\n",
       " 554: 'tn',\n",
       " 555: 'to',\n",
       " 556: 'tp',\n",
       " 557: 'tq',\n",
       " 558: 'tr',\n",
       " 559: 'ts',\n",
       " 560: 'tt',\n",
       " 561: 'tu',\n",
       " 562: 'tv',\n",
       " 563: 'tw',\n",
       " 564: 'tx',\n",
       " 565: 'ty',\n",
       " 566: 'tz',\n",
       " 567: 'u.',\n",
       " 568: 'ua',\n",
       " 569: 'ub',\n",
       " 570: 'uc',\n",
       " 571: 'ud',\n",
       " 572: 'ue',\n",
       " 573: 'uf',\n",
       " 574: 'ug',\n",
       " 575: 'uh',\n",
       " 576: 'ui',\n",
       " 577: 'uj',\n",
       " 578: 'uk',\n",
       " 579: 'ul',\n",
       " 580: 'um',\n",
       " 581: 'un',\n",
       " 582: 'uo',\n",
       " 583: 'up',\n",
       " 584: 'uq',\n",
       " 585: 'ur',\n",
       " 586: 'us',\n",
       " 587: 'ut',\n",
       " 588: 'uu',\n",
       " 589: 'uv',\n",
       " 590: 'uw',\n",
       " 591: 'ux',\n",
       " 592: 'uy',\n",
       " 593: 'uz',\n",
       " 594: 'v.',\n",
       " 595: 'va',\n",
       " 596: 'vb',\n",
       " 597: 'vc',\n",
       " 598: 'vd',\n",
       " 599: 've',\n",
       " 600: 'vf',\n",
       " 601: 'vg',\n",
       " 602: 'vh',\n",
       " 603: 'vi',\n",
       " 604: 'vj',\n",
       " 605: 'vk',\n",
       " 606: 'vl',\n",
       " 607: 'vm',\n",
       " 608: 'vn',\n",
       " 609: 'vo',\n",
       " 610: 'vp',\n",
       " 611: 'vq',\n",
       " 612: 'vr',\n",
       " 613: 'vs',\n",
       " 614: 'vt',\n",
       " 615: 'vu',\n",
       " 616: 'vv',\n",
       " 617: 'vw',\n",
       " 618: 'vx',\n",
       " 619: 'vy',\n",
       " 620: 'vz',\n",
       " 621: 'w.',\n",
       " 622: 'wa',\n",
       " 623: 'wb',\n",
       " 624: 'wc',\n",
       " 625: 'wd',\n",
       " 626: 'we',\n",
       " 627: 'wf',\n",
       " 628: 'wg',\n",
       " 629: 'wh',\n",
       " 630: 'wi',\n",
       " 631: 'wj',\n",
       " 632: 'wk',\n",
       " 633: 'wl',\n",
       " 634: 'wm',\n",
       " 635: 'wn',\n",
       " 636: 'wo',\n",
       " 637: 'wp',\n",
       " 638: 'wq',\n",
       " 639: 'wr',\n",
       " 640: 'ws',\n",
       " 641: 'wt',\n",
       " 642: 'wu',\n",
       " 643: 'wv',\n",
       " 644: 'ww',\n",
       " 645: 'wx',\n",
       " 646: 'wy',\n",
       " 647: 'wz',\n",
       " 648: 'x.',\n",
       " 649: 'xa',\n",
       " 650: 'xb',\n",
       " 651: 'xc',\n",
       " 652: 'xd',\n",
       " 653: 'xe',\n",
       " 654: 'xf',\n",
       " 655: 'xg',\n",
       " 656: 'xh',\n",
       " 657: 'xi',\n",
       " 658: 'xj',\n",
       " 659: 'xk',\n",
       " 660: 'xl',\n",
       " 661: 'xm',\n",
       " 662: 'xn',\n",
       " 663: 'xo',\n",
       " 664: 'xp',\n",
       " 665: 'xq',\n",
       " 666: 'xr',\n",
       " 667: 'xs',\n",
       " 668: 'xt',\n",
       " 669: 'xu',\n",
       " 670: 'xv',\n",
       " 671: 'xw',\n",
       " 672: 'xx',\n",
       " 673: 'xy',\n",
       " 674: 'xz',\n",
       " 675: 'y.',\n",
       " 676: 'ya',\n",
       " 677: 'yb',\n",
       " 678: 'yc',\n",
       " 679: 'yd',\n",
       " 680: 'ye',\n",
       " 681: 'yf',\n",
       " 682: 'yg',\n",
       " 683: 'yh',\n",
       " 684: 'yi',\n",
       " 685: 'yj',\n",
       " 686: 'yk',\n",
       " 687: 'yl',\n",
       " 688: 'ym',\n",
       " 689: 'yn',\n",
       " 690: 'yo',\n",
       " 691: 'yp',\n",
       " 692: 'yq',\n",
       " 693: 'yr',\n",
       " 694: 'ys',\n",
       " 695: 'yt',\n",
       " 696: 'yu',\n",
       " 697: 'yv',\n",
       " 698: 'yw',\n",
       " 699: 'yx',\n",
       " 700: 'yy',\n",
       " 701: 'yz',\n",
       " 702: 'z.',\n",
       " 703: 'za',\n",
       " 704: 'zb',\n",
       " 705: 'zc',\n",
       " 706: 'zd',\n",
       " 707: 'ze',\n",
       " 708: 'zf',\n",
       " 709: 'zg',\n",
       " 710: 'zh',\n",
       " 711: 'zi',\n",
       " 712: 'zj',\n",
       " 713: 'zk',\n",
       " 714: 'zl',\n",
       " 715: 'zm',\n",
       " 716: 'zn',\n",
       " 717: 'zo',\n",
       " 718: 'zp',\n",
       " 719: 'zq',\n",
       " 720: 'zr',\n",
       " 721: 'zs',\n",
       " 722: 'zt',\n",
       " 723: 'zu',\n",
       " 724: 'zv',\n",
       " 725: 'zw',\n",
       " 726: 'zx',\n",
       " 727: 'zy',\n",
       " 728: 'zz'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_s_to_i = {char_tuple: i for i, char_tuple in enumerate(char_tuples)}\n",
    "big_i_to_s = {i: s for s, i in big_s_to_i.items()}\n",
    "big_i_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_s_to_i = {char: i for i, char in enumerate(chars)}\n",
    "tiny_i_to_s = {i: s for s, i in tiny_s_to_i.items()}\n",
    "tiny_i_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chars = [\".\"]*2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chars, chars[1:], chars[2:]):\n",
    "        best_friends = ch1 + ch2\n",
    "        ix1 = big_s_to_i[best_friends]\n",
    "        ix2 = tiny_s_to_i[ch3]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(seed)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "tiny_i_to_s[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306,  ...,  134,  535,  929],\n",
       "        [   0,  207,  190,  ...,   27,  173,  152],\n",
       "        [   0,  169,    0,  ...,    0,    4,    0],\n",
       "        ...,\n",
       "        [   1,    0,    0,  ...,    0,    0,    0],\n",
       "        [  34,   27,    0,  ...,    0,    0,    1],\n",
       "        [   4,   13,    0,  ...,    0,    7,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'junide'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilites = torch.zeros((27,27,27), dtype=torch.float32)\n",
    "# for i in range(27):\n",
    "#     probabilites[i] = N[i].float() / N[i].sum().float()\n",
    "\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "i1x = 0\n",
    "i2x = None\n",
    "char_list = []\n",
    "while i2x != 0 or not char_list:\n",
    "    i2x = torch.multinomial(P[i1x], num_samples=1, replacement=True, generator=g).item()\n",
    "    new_chars = big_i_to_s[i1x][1] + tiny_i_to_s[i2x]\n",
    "    char_list.append(tiny_i_to_s[i2x])\n",
    "    i1x = big_s_to_i[new_chars]\n",
    "\n",
    "\n",
    "''.join(char_list[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Maximize likelihood of data w.r.t. model parameters (statistical model)\n",
    "# Equivalent to maximizing log-likelihood (because log is monotonic)\n",
    "# Equivalent to minimizing negative log-likelihood\n",
    "# Equivalent to minimizing the average negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-22.4022)\n",
      "nll=tensor(22.4022)\n",
      "2.8002805709838867\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "# for w in words:\n",
    "for w in [\"andrejq\"]:\n",
    "    chars = [\".\"]*2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chars, chars[1:], chars[2:]):\n",
    "        best_friends = ch1 + ch2\n",
    "        ix1 = big_s_to_i[best_friends]\n",
    "        ix2 = tiny_s_to_i[ch3]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
    "print(f\"{log_likelihood=}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"{nll=}\")\n",
    "print(f\"{nll/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for bigram:\n",
    "```\n",
    "log_likelihood=tensor(-27.8672)\n",
    "nll=tensor(27.8672)\n",
    "3.4834020137786865\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Borrowed answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    # print(chs, chs[1:])\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 207, 190,  31, 366,  55,  21,  17,  91, 154,  27,  75, 632, 384,\n",
       "        623,  10,  17,   9, 482, 194,  72, 152, 243,   6,  27, 173, 152],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N[0][1].float()    #tri-gram, so it takes 2 letters or 2 int here.\n",
    "p = p / p.sum()\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(seed)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6064, 0.3033, 0.0903])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(seed)\n",
    "p = torch.rand(3, generator=g)\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "ilyasid.\n",
      "prelay.\n",
      "ocin.\n",
      "fairritoper.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix, jx = 0, 0\n",
    "    while True:\n",
    "      p = P[ix][jx]\n",
    "      next_letter = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "      ix = jx\n",
    "      jx = next_letter\n",
    "      out.append(itos[next_letter])\n",
    "      if next_letter == 0:\n",
    "        break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-410414.96875\n",
      "410414.96875\n",
      "2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "#for w in [\"andrejq\"]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    prob = P[ix1, ix2, ix3]\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    n += 1\n",
    "    #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal word:  ['emma']\n",
      ". e m\n",
      "e m m\n",
      "m m a\n",
      "m a .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of tr-igrams (x,y)\n",
    "xs, ys = [], []\n",
    "print(\"Orginal word: \", words[:1])\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    print(ch1, ch2, ch3)\n",
    "    xs.append([ix1, ix2])\n",
    "    ys.append(ix3)\n",
    "\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5],\n",
       "        [ 5, 13],\n",
       "        [13, 13],\n",
       "        [13,  1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13,  1,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc: torch.Tensor = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([4, 2, 27]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype, xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27*2, 1))\n",
    "out1 = xenc.view(4, -1) @ W\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc.view(4, -1) @ W # log-counts\n",
    "counts = logits.exp() # equivalent N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2D tensor with 4 rows and 3 columns\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9],\n",
    "                       [10, 11, 12]])\n",
    "\n",
    "# Use view to reshape it to 3 rows and 4 columns\n",
    "reshaped_tensor = tensor.view(3, 4)\n",
    "\n",
    "# The data stays the same, but the shape is different\n",
    "print(reshaped_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc.view(-1, 27*2) @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 27])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc.view(4, 2*27) @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(4), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0953)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append([ix1, ix2])\n",
    "    ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2735965251922607\n",
      "2.2732994556427\n",
      "2.2730088233947754\n",
      "2.272723436355591\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc.view(-1, 27*2) @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()    #divided by 2 because we have a bigram here\n",
    "  if(k>95):\n",
    "    print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uunidmduauagaduuuufaauaunuamuuinltoliauuauuumuuamnaauuamulemaasudbduinrwiuulusnouinaulautuuunvuumruafoutuuuuufuusuuauruauuuuoraayaeuouuuauaauumuuufmuuimxyninuagaasnumufiuuusuxhxdgoufuuuauuiuu.\n",
      "ujuuuluuumu.\n",
      "rtroeri.\n",
      "iaa.\n",
      "oyei.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  iy = random.randint(1, 26)\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix, iy]), num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27*2) @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 27 x 54 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "s_to_i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "s_to_i[\".\"] = 0\n",
    "i_to_s = {i: s for s, i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  456292\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = [\".\"] * 2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        input_x_1 = s_to_i[ch1]\n",
    "        input_x_2 = s_to_i[ch2]\n",
    "        output_y = s_to_i[ch3]\n",
    "        xs.append((input_x_1, input_x_2))\n",
    "        ys.append(output_y)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of examples: \", num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456292, 228146)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.nelement(), ys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0,  5],\n",
       "        [ 5, 13],\n",
       "        ...,\n",
       "        [26, 25],\n",
       "        [25, 26],\n",
       "        [26, 24]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.242241859436035\n",
      "2.6331708431243896\n",
      "2.5102314949035645\n",
      "2.4684457778930664\n",
      "2.43998646736145\n",
      "2.4310684204101562\n",
      "2.4142682552337646\n",
      "2.415776014328003\n",
      "2.4012656211853027\n",
      "2.4072725772857666\n"
     ]
    }
   ],
   "source": [
    "y_num = ys.nelement()\n",
    "\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc: torch.Tensor = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n",
    "    logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "    counts = logits.exp()  # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "    loss = -probs[torch.arange(y_num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    if (k % 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juwide.\n",
      "janasaz.\n",
      "pariay.\n",
      "ainn.\n",
      "kai.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        # ----------\n",
    "        # BEFORE:\n",
    "        # p = P[ix]\n",
    "        # ----------\n",
    "        # NOW:\n",
    "        xenc: torch.Tensor = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float()\n",
    "        logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "        counts = logits.exp()  # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "        # ----------\n",
    "\n",
    "        iy = torch.multinomial(\n",
    "            p, num_samples=1, replacement=True, generator=g\n",
    "        ).item()\n",
    "        out.append(i_to_s[iy])\n",
    "        ix1 = ix2 #s_to_i[i_to_s[ix] + i_to_s[new_ix]]\n",
    "        ix2 = iy\n",
    "        if iy == 0:\n",
    "            break\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 729 x 27 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = [\".\"] * 2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        input_x = big_s_to_i[ch1 + ch2]\n",
    "        output_y = tiny_s_to_i[ch3]\n",
    "        xs.append(input_x)\n",
    "        ys.append(output_y)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of examples: \", num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27 * 27).float()  # input to the network: one-hot encoding\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 729])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8028223514556885\n",
      "3.20821475982666\n",
      "2.999692916870117\n",
      "2.8769445419311523\n",
      "2.7955620288848877\n",
      "2.737734079360962\n",
      "2.6941769123077393\n",
      "2.6598358154296875\n",
      "2.6318471431732178\n",
      "2.6084728240966797\n",
      "2.5885837078094482\n",
      "2.5714097023010254\n",
      "2.556401014328003\n",
      "2.5431525707244873\n",
      "2.531357526779175\n",
      "2.5207793712615967\n",
      "2.51123046875\n",
      "2.502561092376709\n",
      "2.494649887084961\n",
      "2.4873976707458496\n",
      "2.4807214736938477\n",
      "2.474552631378174\n",
      "2.46883225440979\n",
      "2.4635117053985596\n",
      "2.458547592163086\n",
      "2.453904151916504\n",
      "2.449549913406372\n",
      "2.4454572200775146\n",
      "2.441601514816284\n",
      "2.437962532043457\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(300):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27 * 27).float()  # input to the network: one-hot encoding\n",
    "    logits = xenc @ W  # predict log-counts\n",
    "    counts = logits.exp()  # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    if (k % 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update with learning rate decay\n",
    "    W.data += -50 * W.grad / (1 + k*0.8 / 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram:\n",
    "```\n",
    "2.6608054637908936\n",
    "2.5711233615875244\n",
    "2.536261796951294\n",
    "2.5184857845306396\n",
    "2.5081682205200195\n",
    "2.501594305038452\n",
    "2.4970998764038086\n",
    "2.4938690662384033\n",
    "2.491464376449585\n",
    "2.489628791809082\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juwjded.\n",
      "anaqah.\n",
      "pxzxqywocnzq.\n",
      "jimrttohcaus.\n",
      "ter.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # ----------\n",
    "        # BEFORE:\n",
    "        # p = P[ix]\n",
    "        # ----------\n",
    "        # NOW:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27 * 27).float()\n",
    "        logits = xenc @ W  # predict log-counts\n",
    "        counts = logits.exp()  # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "        # ----------\n",
    "\n",
    "        new_ix = torch.multinomial(\n",
    "            p, num_samples=1, replacement=True, generator=g\n",
    "        ).item()\n",
    "        out.append(tiny_i_to_s[new_ix])\n",
    "        ix = big_s_to_i[big_i_to_s[ix][1] + tiny_i_to_s[new_ix]]\n",
    "        if new_ix == 0:\n",
    "            break\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set.\n",
    "#### Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "import random\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "tmp_words = words.copy()\n",
    "random.shuffle(tmp_words)\n",
    "\n",
    "# create the dataset\n",
    "train_words, val_words, test_words = (\n",
    "    tmp_words[: int(0.8 * len(tmp_words))],\n",
    "    tmp_words[int(0.8 * len(tmp_words)) : int(0.9 * len(tmp_words))],\n",
    "    tmp_words[int(0.9 * len(tmp_words)) :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_matrix(words: list[str], is_trigram: bool = True, smoothing_factor: int = 1):\n",
    "    dim = (27, 27, 27) if is_trigram else (27, 27)\n",
    "    N = torch.zeros(dim, dtype=torch.int32)\n",
    "\n",
    "    chars = sorted(list(set(\"\".join(words))))\n",
    "    s_to_i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "    s_to_i[\".\"] = 0\n",
    "    i_to_s = {i: s for s, i in s_to_i.items()}\n",
    "\n",
    "    for w in words:\n",
    "        chars = [\".\"] * (1 + is_trigram) + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chars, chars[1:], chars[1 + is_trigram :]):\n",
    "            ix1 = s_to_i[ch1]\n",
    "            ix2 = s_to_i[ch2]\n",
    "            if is_trigram:\n",
    "                ix3 = s_to_i[ch3]\n",
    "                N[ix1, ix2, ix3] += 1\n",
    "            else:\n",
    "                N[ix1, ix2] += 1\n",
    "\n",
    "    P = (N + smoothing_factor).float()\n",
    "    P /= P.sum(dim=1 + is_trigram, keepdim=True)\n",
    "\n",
    "    return P, s_to_i, i_to_s\n",
    "\n",
    "\n",
    "def get_loss(words: list[str], P, s_to_i, i_to_s, is_trigram: bool = True, silent = False):\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "    for w in words:\n",
    "        chars = [\".\"] * (1 + is_trigram) + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chars, chars[1:], chars[1 + is_trigram :]):\n",
    "            ix1 = s_to_i[ch1]\n",
    "            ix2 = s_to_i[ch2]\n",
    "            if is_trigram:\n",
    "                ix3 = s_to_i[ch3]\n",
    "                prob = P[ix1, ix2, ix3]\n",
    "            else:\n",
    "                prob = P[ix1, ix2]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob\n",
    "            n += 1\n",
    "    nll = -log_likelihood\n",
    "    if not silent:\n",
    "        print(f\"{log_likelihood=}\")\n",
    "        print(f\"{nll=}\")\n",
    "        print(f\"{nll/n}\")\n",
    "    return nll / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-22.4022)\n",
      "nll=tensor(22.4022)\n",
      "2.8002805709838867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.8003)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss([\"andrejq\"], *get_probability_matrix(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-27.8672)\n",
      "nll=tensor(27.8672)\n",
      "3.4834020137786865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.4834)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss([\"andrejq\"], *get_probability_matrix(words, is_trigram=False), is_trigram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll=tensor(504653.)\n",
      "2.2119739055633545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.2120)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_loss(words, *get_probability_matrix(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-559951.5625)\n",
      "nll=tensor(559951.5625)\n",
      "2.4543561935424805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4544)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(words, *get_probability_matrix(words, is_trigram=False), is_trigram=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_2, s_to_i, i_to_s = get_probability_matrix(train_words, is_trigram=False)\n",
    "P_3, s_to_i, i_to_s = get_probability_matrix(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_name='train'\n",
      "log_likelihood=tensor(-448232.8750)\n",
      "nll=tensor(448232.8750)\n",
      "2.455451726913452\n",
      "\n",
      "data_name='val'\n",
      "log_likelihood=tensor(-56076.6719)\n",
      "nll=tensor(56076.6719)\n",
      "2.455195903778076\n",
      "\n",
      "data_name='test'\n",
      "log_likelihood=tensor(-55794.8945)\n",
      "nll=tensor(55794.8945)\n",
      "2.4514453411102295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_set, data_name in [(train_words, \"train\"), (val_words, \"val\"), (test_words, \"test\")]:\n",
    "    print(f\"{data_name=}\")\n",
    "    get_loss(data_set, P_2, s_to_i, i_to_s, is_trigram=False)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_name='train'\n",
      "log_likelihood=tensor(-404414.3438)\n",
      "nll=tensor(404414.3438)\n",
      "2.2154107093811035\n",
      "\n",
      "data_name='val'\n",
      "log_likelihood=tensor(-51308.7266)\n",
      "nll=tensor(51308.7266)\n",
      "2.246441602706909\n",
      "\n",
      "data_name='test'\n",
      "log_likelihood=tensor(-50781.2344)\n",
      "nll=tensor(50781.2344)\n",
      "2.23116135597229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_set, data_name in [(train_words, \"train\"), (val_words, \"val\"), (test_words, \"test\")]:\n",
    "    print(f\"{data_name=}\")\n",
    "    get_loss(data_set, P_3, s_to_i, i_to_s)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model\n",
    "#### i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(nan)\n",
      "nll=tensor(nan)\n",
      "nan\n",
      "log_likelihood=tensor(-51308.7266)\n",
      "nll=tensor(51308.7266)\n",
      "2.246441602706909\n",
      "log_likelihood=tensor(-51739.4609)\n",
      "nll=tensor(51739.4609)\n",
      "2.2653002738952637\n",
      "log_likelihood=tensor(-52128.5117)\n",
      "nll=tensor(52128.5117)\n",
      "2.282334089279175\n",
      "log_likelihood=tensor(-52483.6172)\n",
      "nll=tensor(52483.6172)\n",
      "2.297881603240967\n",
      "log_likelihood=tensor(-52811.7188)\n",
      "nll=tensor(52811.7188)\n",
      "2.312246799468994\n",
      "log_likelihood=tensor(-53116.9766)\n",
      "nll=tensor(53116.9766)\n",
      "2.3256118297576904\n",
      "log_likelihood=tensor(-53403.7148)\n",
      "nll=tensor(53403.7148)\n",
      "2.3381662368774414\n",
      "log_likelihood=tensor(-53674.4766)\n",
      "nll=tensor(53674.4766)\n",
      "2.3500208854675293\n",
      "log_likelihood=tensor(-53930.2500)\n",
      "nll=tensor(53930.2500)\n",
      "2.3612194061279297\n",
      "log_likelihood=tensor(-54173.8633)\n",
      "nll=tensor(54173.8633)\n",
      "2.3718855381011963\n",
      "log_likelihood=tensor(-54405.5078)\n",
      "nll=tensor(54405.5078)\n",
      "2.3820273876190186\n",
      "log_likelihood=tensor(-54626.9805)\n",
      "nll=tensor(54626.9805)\n",
      "2.391724109649658\n",
      "log_likelihood=tensor(-54840.5547)\n",
      "nll=tensor(54840.5547)\n",
      "2.4010751247406006\n",
      "log_likelihood=tensor(-55045.6758)\n",
      "nll=tensor(55045.6758)\n",
      "2.4100558757781982\n",
      "log_likelihood=tensor(-55242.7734)\n",
      "nll=tensor(55242.7734)\n",
      "2.4186854362487793\n",
      "log_likelihood=tensor(-55432.7539)\n",
      "nll=tensor(55432.7539)\n",
      "2.4270031452178955\n",
      "log_likelihood=tensor(-55616.1992)\n",
      "nll=tensor(55616.1992)\n",
      "2.435034990310669\n",
      "log_likelihood=tensor(-55794.1250)\n",
      "nll=tensor(55794.1250)\n",
      "2.4428250789642334\n",
      "log_likelihood=tensor(-55965.8906)\n",
      "nll=tensor(55965.8906)\n",
      "2.450345516204834\n",
      "log_likelihood=tensor(-56132.4336)\n",
      "nll=tensor(56132.4336)\n",
      "2.457637310028076\n",
      "log_likelihood=tensor(-56293.9883)\n",
      "nll=tensor(56293.9883)\n",
      "2.4647104740142822\n",
      "log_likelihood=tensor(-56451.3203)\n",
      "nll=tensor(56451.3203)\n",
      "2.4715988636016846\n",
      "log_likelihood=tensor(-56604.3594)\n",
      "nll=tensor(56604.3594)\n",
      "2.478299379348755\n",
      "log_likelihood=tensor(-56752.5547)\n",
      "nll=tensor(56752.5547)\n",
      "2.484787940979004\n",
      "log_likelihood=tensor(-56896.6680)\n",
      "nll=tensor(56896.6680)\n",
      "2.4910974502563477\n",
      "log_likelihood=tensor(-57038.5312)\n",
      "nll=tensor(57038.5312)\n",
      "2.4973087310791016\n",
      "log_likelihood=tensor(-57176.4688)\n",
      "nll=tensor(57176.4688)\n",
      "2.5033481121063232\n",
      "log_likelihood=tensor(-57310.9453)\n",
      "nll=tensor(57310.9453)\n",
      "2.5092358589172363\n",
      "log_likelihood=tensor(-57440.5664)\n",
      "nll=tensor(57440.5664)\n",
      "2.514910936355591\n",
      "log_likelihood=tensor(-57569.8945)\n",
      "nll=tensor(57569.8945)\n",
      "2.520573377609253\n",
      "log_likelihood=tensor(-57695.2188)\n",
      "nll=tensor(57695.2188)\n",
      "2.5260603427886963\n",
      "log_likelihood=tensor(-57816.6328)\n",
      "nll=tensor(57816.6328)\n",
      "2.5313761234283447\n",
      "log_likelihood=tensor(-57936.4961)\n",
      "nll=tensor(57936.4961)\n",
      "2.5366241931915283\n",
      "log_likelihood=tensor(-58053.7383)\n",
      "nll=tensor(58053.7383)\n",
      "2.541757345199585\n",
      "log_likelihood=tensor(-58169.0430)\n",
      "nll=tensor(58169.0430)\n",
      "2.5468056201934814\n",
      "log_likelihood=tensor(-58280.0312)\n",
      "nll=tensor(58280.0312)\n",
      "2.5516650676727295\n",
      "log_likelihood=tensor(-58390.3516)\n",
      "nll=tensor(58390.3516)\n",
      "2.556495189666748\n",
      "log_likelihood=tensor(-58498.1133)\n",
      "nll=tensor(58498.1133)\n",
      "2.561213254928589\n",
      "log_likelihood=tensor(-58604.2812)\n",
      "nll=tensor(58604.2812)\n",
      "2.565861701965332\n",
      "log_likelihood=tensor(-58708.4102)\n",
      "nll=tensor(58708.4102)\n",
      "2.570420742034912\n",
      "log_likelihood=tensor(-58810.3125)\n",
      "nll=tensor(58810.3125)\n",
      "2.5748822689056396\n",
      "log_likelihood=tensor(-58910.1484)\n",
      "nll=tensor(58910.1484)\n",
      "2.5792534351348877\n",
      "log_likelihood=tensor(-59008.9727)\n",
      "nll=tensor(59008.9727)\n",
      "2.583580255508423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb Cell 111\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1000\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     P, s_to_i, i_to_s \u001b[39m=\u001b[39m get_probability_matrix(train_words, smoothing_factor\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(get_loss(val_words, P, s_to_i, i_to_s))\n",
      "\u001b[1;32m/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb Cell 111\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m is_trigram:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     ix3 \u001b[39m=\u001b[39m s_to_i[ch3]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     N[ix1, ix2, ix3] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y215sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     N[ix1, ix2] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(0, 1000):\n",
    "    P, s_to_i, i_to_s = get_probability_matrix(train_words, smoothing_factor=i)\n",
    "    losses.append(get_loss(val_words, P, s_to_i, i_to_s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "# get the best smoothing factor\n",
    "best_smoothing_factor = torch.tensor(losses).argmin().item()\n",
    "best_smoothing_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "s_to_i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "s_to_i[\".\"] = 0\n",
    "i_to_s = {i: s for s, i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_words(words: list[str], s_to_i: dict[str, int]):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        chs = [\".\"] * 2 + list(w) + [\".\"]\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            input_x_1 = s_to_i[ch1]\n",
    "            input_x_2 = s_to_i[ch2]\n",
    "            output_y = s_to_i[ch3]\n",
    "            xs.append((input_x_1, input_x_2))\n",
    "            ys.append(output_y)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    print(\"number of examples: \", ys.nelement())\n",
    "    return xs, ys\n",
    "\n",
    "def train_nn(xs: torch.Tensor, ys: torch.Tensor, regularization: float = 0.01):\n",
    "    # initialize the 'network'\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    y_num = ys.nelement()\n",
    "\n",
    "    # gradient descent\n",
    "    for k in range(100):\n",
    "        # forward pass\n",
    "        xenc: torch.Tensor = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n",
    "        logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "        counts = logits.exp()  # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "        loss: torch.Tensor = -probs[torch.arange(y_num), ys].log().mean() + regularization * (W**2).mean()\n",
    "\n",
    "        if (k % 20) == 0:\n",
    "            print(loss.item())\n",
    "        # backward pass\n",
    "        W.grad = None  # set to zero the gradient\n",
    "        loss.backward()\n",
    "        # update\n",
    "        W.data += -50 * W.grad\n",
    "    return W\n",
    "\n",
    "def compute_loss(W: torch.Tensor, xs: torch.Tensor, ys: torch.Tensor, regularization: float = 0.01):\n",
    "    y_num = ys.nelement()\n",
    "    xenc: torch.Tensor = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n",
    "    logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "    counts = logits.exp()  # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "    loss: torch.Tensor = -probs[torch.arange(y_num), ys].log().mean() + regularization * (W**2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  182546\n",
      "number of examples:  22840\n",
      "number of examples:  22760\n"
     ]
    }
   ],
   "source": [
    "train_xs, train_ys = dataset_from_words(train_words, s_to_i)\n",
    "val_xs, val_ys = dataset_from_words(val_words, s_to_i)\n",
    "test_xs, test_ys = dataset_from_words(test_words, s_to_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: 0.0\n",
      "4.2332444190979\n",
      "2.5049571990966797\n",
      "2.4339613914489746\n",
      "2.4076805114746094\n",
      "2.393829107284546\n",
      "tensor(2.3876, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.01\n",
      "4.242944717407227\n",
      "2.5112154483795166\n",
      "2.440770387649536\n",
      "2.414987325668335\n",
      "2.4017438888549805\n",
      "tensor(2.3961, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.02\n",
      "4.252645015716553\n",
      "2.5172061920166016\n",
      "2.44711971282959\n",
      "2.421684980392456\n",
      "2.4089560508728027\n",
      "tensor(2.4037, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.03\n",
      "4.262345790863037\n",
      "2.5229427814483643\n",
      "2.453049659729004\n",
      "2.427856683731079\n",
      "2.4155702590942383\n",
      "tensor(2.4106, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.04\n",
      "4.272046089172363\n",
      "2.5284337997436523\n",
      "2.4585916996002197\n",
      "2.43357253074646\n",
      "2.42167329788208\n",
      "tensor(2.4169, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.05\n",
      "4.2817463874816895\n",
      "2.533688545227051\n",
      "2.4637739658355713\n",
      "2.438890218734741\n",
      "2.4273335933685303\n",
      "tensor(2.4228, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.06\n",
      "4.291446685791016\n",
      "2.53872013092041\n",
      "2.46863055229187\n",
      "2.4438679218292236\n",
      "2.432612419128418\n",
      "tensor(2.4282, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.07\n",
      "4.3011474609375\n",
      "2.5435352325439453\n",
      "2.4731855392456055\n",
      "2.4485483169555664\n",
      "2.4375617504119873\n",
      "tensor(2.4332, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.08\n",
      "4.310847759246826\n",
      "2.5481417179107666\n",
      "2.477465867996216\n",
      "2.4529683589935303\n",
      "2.4422266483306885\n",
      "tensor(2.4380, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.09\n",
      "4.320548057556152\n",
      "2.55255389213562\n",
      "2.481498956680298\n",
      "2.457160711288452\n",
      "2.4466445446014404\n",
      "tensor(2.4425, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.1\n",
      "4.3302483558654785\n",
      "2.556772232055664\n",
      "2.4852986335754395\n",
      "2.4611494541168213\n",
      "2.45084810256958\n",
      "tensor(2.4469, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.11\n",
      "4.339949131011963\n",
      "2.560812473297119\n",
      "2.4888980388641357\n",
      "2.464958429336548\n",
      "2.454867362976074\n",
      "tensor(2.4510, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.12\n",
      "4.349649429321289\n",
      "2.5646753311157227\n",
      "2.4923095703125\n",
      "2.4686050415039062\n",
      "2.4587254524230957\n",
      "tensor(2.4550, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.13\n",
      "4.359349727630615\n",
      "2.5683743953704834\n",
      "2.495556354522705\n",
      "2.4721083641052246\n",
      "2.462442636489868\n",
      "tensor(2.4589, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.14\n",
      "4.369050025939941\n",
      "2.571911334991455\n",
      "2.4986507892608643\n",
      "2.4754810333251953\n",
      "2.4660377502441406\n",
      "tensor(2.4626, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.15\n",
      "4.378750801086426\n",
      "2.5752971172332764\n",
      "2.5016143321990967\n",
      "2.4787397384643555\n",
      "2.4695241451263428\n",
      "tensor(2.4662, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.16\n",
      "4.388451099395752\n",
      "2.5785367488861084\n",
      "2.504457712173462\n",
      "2.4818925857543945\n",
      "2.4729154109954834\n",
      "tensor(2.4698, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.17\n",
      "4.398151397705078\n",
      "2.5816385746002197\n",
      "2.507194995880127\n",
      "2.484952449798584\n",
      "2.4762229919433594\n",
      "tensor(2.4733, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.18\n",
      "4.407851696014404\n",
      "2.5846054553985596\n",
      "2.5098352432250977\n",
      "2.487929344177246\n",
      "2.4794540405273438\n",
      "tensor(2.4767, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.19\n",
      "4.4175519943237305\n",
      "2.5874454975128174\n",
      "2.5123915672302246\n",
      "2.4908299446105957\n",
      "2.482616424560547\n",
      "tensor(2.4800, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.2\n",
      "4.427252769470215\n",
      "2.5901620388031006\n",
      "2.5148696899414062\n",
      "2.4936633110046387\n",
      "2.485718011856079\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.21\n",
      "4.436953067779541\n",
      "2.5927658081054688\n",
      "2.517279863357544\n",
      "2.4964356422424316\n",
      "2.488762617111206\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.22\n",
      "4.446653366088867\n",
      "2.5952577590942383\n",
      "2.5196268558502197\n",
      "2.499152421951294\n",
      "2.4917547702789307\n",
      "tensor(2.4896, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.23\n",
      "4.456353664398193\n",
      "2.597641706466675\n",
      "2.521916627883911\n",
      "2.5018205642700195\n",
      "2.494699478149414\n",
      "tensor(2.4927, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.24\n",
      "4.466054439544678\n",
      "2.5999269485473633\n",
      "2.524153709411621\n",
      "2.5044422149658203\n",
      "2.497598886489868\n",
      "tensor(2.4958, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.25\n",
      "4.475754737854004\n",
      "2.60211443901062\n",
      "2.5263428688049316\n",
      "2.5070230960845947\n",
      "2.500455856323242\n",
      "tensor(2.4988, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.26\n",
      "4.48545503616333\n",
      "2.6042113304138184\n",
      "2.5284879207611084\n",
      "2.5095651149749756\n",
      "2.5032730102539062\n",
      "tensor(2.5017, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.27\n",
      "4.495155334472656\n",
      "2.606220245361328\n",
      "2.5305914878845215\n",
      "2.512073040008545\n",
      "2.5060524940490723\n",
      "tensor(2.5046, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.28\n",
      "4.504856109619141\n",
      "2.6081478595733643\n",
      "2.5326578617095947\n",
      "2.5145487785339355\n",
      "2.5087954998016357\n",
      "tensor(2.5075, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.29\n",
      "4.514556407928467\n",
      "2.609994888305664\n",
      "2.5346884727478027\n",
      "2.516993761062622\n",
      "2.511503219604492\n",
      "tensor(2.5103, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.3\n",
      "4.524256706237793\n",
      "2.6117663383483887\n",
      "2.5366873741149902\n",
      "2.519411087036133\n",
      "2.514177083969116\n",
      "tensor(2.5131, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.31\n",
      "4.533957004547119\n",
      "2.6134681701660156\n",
      "2.538655996322632\n",
      "2.5218021869659424\n",
      "2.5168190002441406\n",
      "tensor(2.5158, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.32\n",
      "4.543657302856445\n",
      "2.6151013374328613\n",
      "2.5405969619750977\n",
      "2.524167537689209\n",
      "2.5194292068481445\n",
      "tensor(2.5185, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.33\n",
      "4.55335807800293\n",
      "2.6166694164276123\n",
      "2.5425119400024414\n",
      "2.526510000228882\n",
      "2.5220084190368652\n",
      "tensor(2.5212, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.34\n",
      "4.563058376312256\n",
      "2.618177652359009\n",
      "2.5444042682647705\n",
      "2.52882981300354\n",
      "2.524557113647461\n",
      "tensor(2.5238, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.35\n",
      "4.572758674621582\n",
      "2.6196277141571045\n",
      "2.546273946762085\n",
      "2.531127452850342\n",
      "2.5270767211914062\n",
      "tensor(2.5264, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.36\n",
      "4.582458972930908\n",
      "2.6210222244262695\n",
      "2.548123836517334\n",
      "2.5334043502807617\n",
      "2.5295674800872803\n",
      "tensor(2.5290, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.37\n",
      "4.592159748077393\n",
      "2.622366428375244\n",
      "2.549955368041992\n",
      "2.535660743713379\n",
      "2.5320303440093994\n",
      "tensor(2.5315, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.38\n",
      "4.601860046386719\n",
      "2.623663902282715\n",
      "2.551769733428955\n",
      "2.537898302078247\n",
      "2.5344650745391846\n",
      "tensor(2.5340, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.39\n",
      "4.611560344696045\n",
      "2.6249117851257324\n",
      "2.5535686016082764\n",
      "2.5401155948638916\n",
      "2.5368733406066895\n",
      "tensor(2.5365, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.4\n",
      "4.621260643005371\n",
      "2.6261205673217773\n",
      "2.5553529262542725\n",
      "2.542315721511841\n",
      "2.5392541885375977\n",
      "tensor(2.5389, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.41\n",
      "4.6309614181518555\n",
      "2.627284288406372\n",
      "2.5571231842041016\n",
      "2.544497013092041\n",
      "2.541609287261963\n",
      "tensor(2.5413, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.42\n",
      "4.640661716461182\n",
      "2.6284127235412598\n",
      "2.5588808059692383\n",
      "2.546659469604492\n",
      "2.543938398361206\n",
      "tensor(2.5437, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.43\n",
      "4.650362014770508\n",
      "2.6295037269592285\n",
      "2.5606274604797363\n",
      "2.5488057136535645\n",
      "2.5462417602539062\n",
      "tensor(2.5460, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.44\n",
      "4.660062313079834\n",
      "2.6305627822875977\n",
      "2.5623626708984375\n",
      "2.550934076309204\n",
      "2.548520803451538\n",
      "tensor(2.5483, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.45\n",
      "4.66976261138916\n",
      "2.631587266921997\n",
      "2.564087390899658\n",
      "2.5530452728271484\n",
      "2.5507750511169434\n",
      "tensor(2.5506, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.46\n",
      "4.6794633865356445\n",
      "2.632584810256958\n",
      "2.565803050994873\n",
      "2.5551388263702393\n",
      "2.5530049800872803\n",
      "tensor(2.5528, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.47\n",
      "4.689163684844971\n",
      "2.6335554122924805\n",
      "2.5675079822540283\n",
      "2.557216167449951\n",
      "2.5552115440368652\n",
      "tensor(2.5551, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.48\n",
      "4.698863983154297\n",
      "2.6344985961914062\n",
      "2.5692059993743896\n",
      "2.559277057647705\n",
      "2.5573952198028564\n",
      "tensor(2.5573, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.49\n",
      "4.708564758300781\n",
      "2.6354196071624756\n",
      "2.570894956588745\n",
      "2.5613210201263428\n",
      "2.5595550537109375\n",
      "tensor(2.5594, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.5\n",
      "4.718265056610107\n",
      "2.636319637298584\n",
      "2.572575092315674\n",
      "2.5633487701416016\n",
      "2.561692714691162\n",
      "tensor(2.5616, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.51\n",
      "4.727965354919434\n",
      "2.6371970176696777\n",
      "2.5742475986480713\n",
      "2.5653607845306396\n",
      "2.5638086795806885\n",
      "tensor(2.5637, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.52\n",
      "4.73766565322876\n",
      "2.63805890083313\n",
      "2.575913429260254\n",
      "2.567356824874878\n",
      "2.5659029483795166\n",
      "tensor(2.5658, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.53\n",
      "4.747365951538086\n",
      "2.6389012336730957\n",
      "2.577571153640747\n",
      "2.5693368911743164\n",
      "2.5679750442504883\n",
      "tensor(2.5679, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.54\n",
      "4.75706672668457\n",
      "2.6397287845611572\n",
      "2.5792222023010254\n",
      "2.571300983428955\n",
      "2.570026397705078\n",
      "tensor(2.5699, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.55\n",
      "4.7667670249938965\n",
      "2.6405417919158936\n",
      "2.5808660984039307\n",
      "2.573249340057373\n",
      "2.5720572471618652\n",
      "tensor(2.5720, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.56\n",
      "4.776467323303223\n",
      "2.641343832015991\n",
      "2.5825037956237793\n",
      "2.5751824378967285\n",
      "2.5740673542022705\n",
      "tensor(2.5740, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.57\n",
      "4.786167621612549\n",
      "2.642131805419922\n",
      "2.5841329097747803\n",
      "2.5771005153656006\n",
      "2.5760576725006104\n",
      "tensor(2.5760, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.58\n",
      "4.795867919921875\n",
      "2.642911195755005\n",
      "2.5857579708099365\n",
      "2.5790023803710938\n",
      "2.578029155731201\n",
      "tensor(2.5779, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.59\n",
      "4.805568695068359\n",
      "2.6436800956726074\n",
      "2.5873754024505615\n",
      "2.5808889865875244\n",
      "2.5799806118011475\n",
      "tensor(2.5799, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.6\n",
      "4.8152689933776855\n",
      "2.6444406509399414\n",
      "2.588986396789551\n",
      "2.5827622413635254\n",
      "2.581913471221924\n",
      "tensor(2.5818, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.61\n",
      "4.824969291687012\n",
      "2.645195484161377\n",
      "2.5905909538269043\n",
      "2.5846195220947266\n",
      "2.5838277339935303\n",
      "tensor(2.5837, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.62\n",
      "4.834670066833496\n",
      "2.6459436416625977\n",
      "2.5921900272369385\n",
      "2.5864620208740234\n",
      "2.585723400115967\n",
      "tensor(2.5856, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.63\n",
      "4.844370365142822\n",
      "2.6466867923736572\n",
      "2.593782424926758\n",
      "2.588290214538574\n",
      "2.587601661682129\n",
      "tensor(2.5875, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.64\n",
      "4.854070663452148\n",
      "2.6474244594573975\n",
      "2.5953691005706787\n",
      "2.5901038646698\n",
      "2.5894615650177\n",
      "tensor(2.5893, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.65\n",
      "4.863770961761475\n",
      "2.6481595039367676\n",
      "2.596949577331543\n",
      "2.5919029712677\n",
      "2.5913050174713135\n",
      "tensor(2.5912, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.66\n",
      "4.873471260070801\n",
      "2.6488921642303467\n",
      "2.5985231399536133\n",
      "2.5936880111694336\n",
      "2.5931310653686523\n",
      "tensor(2.5930, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.67\n",
      "4.883172035217285\n",
      "2.6496224403381348\n",
      "2.6000924110412598\n",
      "2.595459461212158\n",
      "2.594940423965454\n",
      "tensor(2.5948, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.68\n",
      "4.892872333526611\n",
      "2.650351047515869\n",
      "2.601654291152954\n",
      "2.5972166061401367\n",
      "2.596734046936035\n",
      "tensor(2.5966, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.69\n",
      "4.9025726318359375\n",
      "2.6510791778564453\n",
      "2.60321044921875\n",
      "2.598958730697632\n",
      "2.598511219024658\n",
      "tensor(2.5983, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.7\n",
      "4.912272930145264\n",
      "2.6518070697784424\n",
      "2.6047608852386475\n",
      "2.600689172744751\n",
      "2.6002724170684814\n",
      "tensor(2.6001, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.71\n",
      "4.92197322845459\n",
      "2.6525354385375977\n",
      "2.606304407119751\n",
      "2.602405071258545\n",
      "2.602017879486084\n",
      "tensor(2.6018, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.72\n",
      "4.931674003601074\n",
      "2.6532645225524902\n",
      "2.6078426837921143\n",
      "2.60410737991333\n",
      "2.603748321533203\n",
      "tensor(2.6035, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.73\n",
      "4.9413743019104\n",
      "2.653994560241699\n",
      "2.6093735694885254\n",
      "2.6057960987091064\n",
      "2.605463981628418\n",
      "tensor(2.6053, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.74\n",
      "4.951074600219727\n",
      "2.6547272205352783\n",
      "2.610898733139038\n",
      "2.6074724197387695\n",
      "2.6071646213531494\n",
      "tensor(2.6069, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.75\n",
      "4.960775375366211\n",
      "2.655461072921753\n",
      "2.612417221069336\n",
      "2.609135150909424\n",
      "2.608851671218872\n",
      "tensor(2.6086, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.76\n",
      "4.970475673675537\n",
      "2.6561977863311768\n",
      "2.613929510116577\n",
      "2.610785722732544\n",
      "2.6105237007141113\n",
      "tensor(2.6103, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.77\n",
      "4.980175971984863\n",
      "2.656937837600708\n",
      "2.615434169769287\n",
      "2.612422466278076\n",
      "2.612182378768921\n",
      "tensor(2.6119, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.78\n",
      "4.9898762702941895\n",
      "2.657679796218872\n",
      "2.6169326305389404\n",
      "2.614046812057495\n",
      "2.6138269901275635\n",
      "tensor(2.6136, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.79\n",
      "4.999576568603516\n",
      "2.6584270000457764\n",
      "2.6184232234954834\n",
      "2.61565899848938\n",
      "2.615457773208618\n",
      "tensor(2.6152, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.8\n",
      "5.00927734375\n",
      "2.6591763496398926\n",
      "2.6199066638946533\n",
      "2.6172592639923096\n",
      "2.617077350616455\n",
      "tensor(2.6168, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.81\n",
      "5.018977642059326\n",
      "2.6599299907684326\n",
      "2.621382474899292\n",
      "2.6188464164733887\n",
      "2.6186816692352295\n",
      "tensor(2.6184, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.82\n",
      "5.028677940368652\n",
      "2.6606876850128174\n",
      "2.6228506565093994\n",
      "2.62042236328125\n",
      "2.620274782180786\n",
      "tensor(2.6200, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.83\n",
      "5.0383782386779785\n",
      "2.661449432373047\n",
      "2.624310255050659\n",
      "2.621987819671631\n",
      "2.621854066848755\n",
      "tensor(2.6215, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.84\n",
      "5.048079013824463\n",
      "2.6622159481048584\n",
      "2.6257612705230713\n",
      "2.6235408782958984\n",
      "2.6234214305877686\n",
      "tensor(2.6231, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.85\n",
      "5.057779312133789\n",
      "2.6629867553710938\n",
      "2.6272029876708984\n",
      "2.6250839233398438\n",
      "2.624976873397827\n",
      "tensor(2.6246, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.86\n",
      "5.067479610443115\n",
      "2.663761615753174\n",
      "2.628636598587036\n",
      "2.626615524291992\n",
      "2.6265199184417725\n",
      "tensor(2.6262, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.87\n",
      "5.077179908752441\n",
      "2.664541006088257\n",
      "2.6300599575042725\n",
      "2.6281378269195557\n",
      "2.6280503273010254\n",
      "tensor(2.6277, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.88\n",
      "5.086880683898926\n",
      "2.665325403213501\n",
      "2.6314733028411865\n",
      "2.629650592803955\n",
      "2.6295697689056396\n",
      "tensor(2.6292, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.89\n",
      "5.096580982208252\n",
      "2.66611385345459\n",
      "2.6328773498535156\n",
      "2.6311542987823486\n",
      "2.6310760974884033\n",
      "tensor(2.6307, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.9\n",
      "5.106281280517578\n",
      "2.6669070720672607\n",
      "2.634270191192627\n",
      "2.6326494216918945\n",
      "2.6325693130493164\n",
      "tensor(2.6322, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.91\n",
      "5.115981578826904\n",
      "2.6677052974700928\n",
      "2.6356518268585205\n",
      "2.6341381072998047\n",
      "2.6340484619140625\n",
      "tensor(2.6336, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.92\n",
      "5.1256818771362305\n",
      "2.6685075759887695\n",
      "2.6370227336883545\n",
      "2.635619878768921\n",
      "2.635514259338379\n",
      "tensor(2.6351, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.93\n",
      "5.135382652282715\n",
      "2.6693148612976074\n",
      "2.6383824348449707\n",
      "2.6370959281921387\n",
      "2.6369664669036865\n",
      "tensor(2.6366, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.94\n",
      "5.145082950592041\n",
      "2.670126438140869\n",
      "2.63973069190979\n",
      "2.6385679244995117\n",
      "2.638401746749878\n",
      "tensor(2.6380, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.95\n",
      "5.154783248901367\n",
      "2.6709401607513428\n",
      "2.641068696975708\n",
      "2.6400365829467773\n",
      "2.639821767807007\n",
      "tensor(2.6395, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.96\n",
      "5.164483547210693\n",
      "2.6717605590820312\n",
      "2.6423938274383545\n",
      "2.641502618789673\n",
      "2.6412224769592285\n",
      "tensor(2.6409, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.97\n",
      "5.174184322357178\n",
      "2.67258358001709\n",
      "2.6437089443206787\n",
      "2.642967939376831\n",
      "2.6426072120666504\n",
      "tensor(2.6424, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.98\n",
      "5.183884620666504\n",
      "2.6734116077423096\n",
      "2.6450119018554688\n",
      "2.6444320678710938\n",
      "2.643969774246216\n",
      "tensor(2.6438, grad_fn=<AddBackward0>)\n",
      "\n",
      "Regularization: 0.99\n",
      "5.19358491897583\n",
      "2.6742420196533203\n",
      "2.646306276321411\n",
      "2.6458964347839355\n",
      "2.6453135013580322\n",
      "tensor(2.6453, grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(100):\n",
    "    print(f\"Regularization: {i/100}\")\n",
    "    W = train_nn(train_xs, train_ys, regularization=i/100)\n",
    "    losses.append(compute_loss(W, val_xs, val_ys, regularization=i/100))\n",
    "    print(losses[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7446e9e40>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([loss.item() for loss in losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful.\n",
    "#### Can you delete our use of F.one_hot in favor of simply indexing into rows of W?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "s_to_i = {s: i + 1 for i, s in enumerate(chars)}\n",
    "s_to_i[\".\"] = 0\n",
    "i_to_s = {i: s for s, i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  456292\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = [\".\"] * 2 + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        input_x_1 = s_to_i[ch1]\n",
    "        input_x_2 = s_to_i[ch2]\n",
    "        output_y = s_to_i[ch3]\n",
    "        xs.append((input_x_1, input_x_2))\n",
    "        ys.append(output_y)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of examples: \", num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456292, 228146)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.nelement(), ys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4997568130493164\n",
      "2.4894626140594482\n",
      "2.4559082984924316\n",
      "2.4694080352783203\n",
      "2.433418035507202\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb Cell 165\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m W\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# set to zero the gradient\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/anthony/personal/karpathy/assignments/building_makemore_1_exercises.ipynb#Y322sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m W\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m50\u001b[39m \u001b[39m*\u001b[39m W\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/karpathy/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/karpathy/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_num = ys.nelement()\n",
    "\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    # xenc: torch.Tensor = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n",
    "    # logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "    W_first_half = torch.stack([W[x[0]] for x in xs])\n",
    "    W_second_half = torch.stack([W[x[1]+27] for x in xs])\n",
    "    logits = W_first_half + W_second_half\n",
    "    # logits = torch.stack([W[x[0]] + W[x[1]+27] for x in xs])\n",
    "    counts = logits.exp()  # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "    loss = -probs[torch.arange(y_num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    if (k % 10) == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juwide.\n",
      "janasaz.\n",
      "alen.\n",
      "amainn.\n",
      "kai.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        # ----------\n",
    "        # BEFORE:\n",
    "        # p = P[ix]\n",
    "        # ----------\n",
    "        # NOW:\n",
    "        xenc: torch.Tensor = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float()\n",
    "        logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "        counts = logits.exp()  # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "        # ----------\n",
    "\n",
    "        iy = torch.multinomial(\n",
    "            p, num_samples=1, replacement=True, generator=g\n",
    "        ).item()\n",
    "        out.append(i_to_s[iy])\n",
    "        ix1 = ix2 #s_to_i[i_to_s[ix] + i_to_s[new_ix]]\n",
    "        ix2 = iy\n",
    "        if iy == 0:\n",
    "            break\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juwide.\n",
    "# janasaz.\n",
    "# pariay.\n",
    "# ainn.\n",
    "# kai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E05: look up and use F.cross_entropy instead. You should achieve the same result.\n",
    "#### Can you think of why we'd prefer to use F.cross_entropy instead?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_nn_cross_entropy(xs: torch.Tensor, ys: torch.Tensor, regularization: float = 0.01):\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "\n",
    "y_num = ys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.232540607452393\n",
      "2.5039820671081543\n",
      "2.433206796646118\n",
      "2.4069669246673584\n",
      "2.3933231830596924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    xenc: torch.Tensor = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n",
    "    logits = xenc.view(-1, 2*27) @ W  # predict log-counts\n",
    "    # counts = logits.exp()  # counts, equivalent to N\n",
    "    # probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n",
    "    # loss: torch.Tensor = -probs[torch.arange(y_num), ys].log().mean() + regularization * (W**2).mean()\n",
    "    loss = F.cross_entropy(logits, ys) #+ regularization * (W**2).mean()\n",
    "\n",
    "\n",
    "    if (k % 20) == 0:\n",
    "        print(loss.item())\n",
    "    # backward pass\n",
    "    W.grad = None  # set to zero the gradient\n",
    "    loss.backward()\n",
    "    # update\n",
    "    W.data += -50 * W.grad\n",
    "# return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of target with class indices\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "# loss = F.cross_entropy(input, target)\n",
    "# loss.backward()\n",
    "# # Example of target with class probabilities\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randn(3, 5).softmax(dim=1)\n",
    "# loss = F.cross_entropy(input, target)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already done with multiple implementations (729 x 27 and 27 x 27 x 27), as well as learning rate decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
